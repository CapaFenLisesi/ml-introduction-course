{"kernelspec":{"display_name":"Python 3 (Anaconda)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 05\n## Big Data Enrichment: Joining and Grouping data\n\nOne of the skills in working with data is to pull multiple data sets together. There are often features we would like to use in training our machine learning model that aren't necessarily in the dataset we have in front of us. For example, we may be looking at attendance figures at baseball games, trying to predict what they will be for future games. However, we only have team statistics. The attendance certainly depends on the weather at the stadium (or in the city) on the day of a game. So we would like to be able to gather the weather data and add it to our model. We'll go through how to do something like that."}
{"cell_type":"markdown","metadata":{},"source":"### First Dataset\n\nWe start with data from the [Iowa open database of class \"E\" liquor licencee purchases](https://data.iowa.gov/Economy/Iowa-Liquor-Sales/m3tr-qhgy/data). I have filtered the data to include data from 10/1/2016 to 12/31/2016. I have also added up the sales for each day to each store. "}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::910fc839-0567-4d99-8a1c-2098f9783d29","text/plain":"smc-blob::bbe78787-8c78-4424-b51e-177ef51b2448"},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":"import pandas as pd\niowadf= pd.read_csv(\"Class05_iowa_data.csv\")\niowadf.head()"}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"Date                              object\nStore Number                       int64\nCity                              object\nZip Code                          object\nCounty                            object\nTotal Sales to Store (Dollars)    object\ndtype: object"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":"# The sales data looks like it isn't a float like we want it to be (the presence of a $ in front is my clue that there may be something wrong.) Let's look at the data types to be sure.\niowadf.dtypes"}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Date                               object\nStore Number                        int64\nCity                               object\nZip Code                           object\nCounty                             object\nTotal Sales to Store (Dollars)     object\nSalesToStore                      float64\ndtype: object\n"},{"data":{"text/html":"smc-blob::eb8d84b3-3926-4dba-bdb7-d14f483ee47d","text/plain":"smc-blob::100c6a25-4dee-4ff4-ba5f-7dab85885348"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"# Sure enough. We need to get the real values from that column. We'll create a new column for that data and use the regex parser to get the number.\niowadf['SalesToStore'] = iowadf['Total Sales to Store (Dollars)'].str.extract(\"\\$(.*)\", expand=True)\n# We also need to convert the output to a float.\niowadf['SalesToStore'] = iowadf['SalesToStore'].astype(float)\nprint(iowadf.dtypes)\niowadf.head()"}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Date                                      object\nStore Number                               int64\nCity                                      object\nZip Code                                  object\nCounty                                    object\nTotal Sales to Store (Dollars)            object\nSalesToStore                             float64\nSalesDate                         datetime64[ns]\ndtype: object\n"},{"data":{"text/html":"smc-blob::d0ed0a5d-ec1b-4fc2-af64-101cad3454ce","text/plain":"smc-blob::dda0c56e-566a-4d7b-8631-fa95c0c5ac25"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"# We also need to convert the date. We'll try this and see if it parses the date correctly.\niowadf[\"SalesDate\"] = pd.to_datetime(iowadf[\"Date\"])\nprint(iowadf.dtypes)\niowadf.head()"}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"True"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"# Let's do a bit more data exploration here to see if there are any other issues. For example, let's see if there are any NA values in the dataframe. That may indicate a problem. We'll first check the entire dataframe.\n# This looks for any null value then combines them together. It will only be true if there are any null values.\niowadf.isnull().values.any()"}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Initial rows: 13779\nCleaned rows: 13754\n"},{"data":{"text/plain":"False"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"#Let's see how many data points have null values. We'll look at the total rows and \nprint(\"Initial rows: {}\".format(len(iowadf.index)))\niowadfcl = iowadf.dropna()\nprint(\"Cleaned rows: {}\".format(len(iowadfcl.index)))\niowadfcl.isnull().values.any()"}
{"cell_type":"markdown","metadata":{},"source":"We only lose 25 rows out of 13,000. I'm going to go with that- it simplifies further computations.\n\n## Grouping and subsetting data\n\nWe now want to narrow down the data in order to make it easier to work with joining datasets. Let's get the top-purchasing store and use just that store for our further work. We first need to do an analysis step: we want to add up all the sales for each store. In other words, we want to group by store, then sum up the StoreSales. \n\nThe first step is to use the `groupby()` function to group the data into sections that all have something in common. For our case, we will group by the \"Store Number\". Pandas will go through that column, find all the unique values, then group them together and return a groupby object. We'll look at the first group to see what is in it."}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[],"source":"storegroups = iowadfcl.groupby('Store Number')"}
{"cell_type":"markdown","metadata":{},"source":"We now want the sum of all of the SalesToStore values in each group. We can do this easily since it is the only numerical value in the group. We apply the `sum()` function of the group to get the sum. There are a handful of other built-in functions. You can get a list of the functions by pressing the `<TAB>` key after typing `storegroups.` Try it to see the list of functions. Documentation for these functions is [provided on the DataFrame document page](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)."}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::a1822294-7dd2-4d9e-8886-1fbeae8252f6","text/plain":"smc-blob::af9672ed-5bdc-473a-9959-18bc36b3014a"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":"storegroups.count()"}
{"cell_type":"markdown","metadata":{},"source":"### Sorting Data\n\nWe want to see the stores with the largest purchases. To do this, we will sort the `SalesToStore` column in descending order (which puts the largest values at the top of the list). We use a pandas function `sort_values()` that takes the column name that you want to sort by as an argument. We also add the argument `ascending=False` to tell pandas to give us the results in descending order."}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::7f9406d5-8996-44b4-b48d-3c7a8634a7a8","text/plain":"              SalesToStore\nStore Number              \n2633             805018.15\n4829             788918.57\n2670             441203.21\n3773             391374.52\n2512             381749.62\n3952             329222.08\n5102             322702.94\n2663             301419.44\n2619             284403.05\n3385             283767.38"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"storesum = storegroups.sum()\n\n# Let's sort this list by the sales number in descending order:\nstorelist = storesum.sort_values('SalesToStore',ascending=False)\n\n# And list the top stores:\nstorelist.head(10)"}
{"cell_type":"markdown","metadata":{},"source":"### Subsetting Data\n\nIt looks like the store number 2633 purchased more liquor than any other store, so we want only that subset of the data. We subset the data by looking only for rows where the store number matches 2633. This returns a column of True/False values where the Store Number matches our store."}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"0    False\n1    False\n2    False\n3    False\n4    False\nName: Store Number, dtype: bool"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"subsetrows = iowadfcl['Store Number']==2633\nsubsetrows.head()"}
{"cell_type":"markdown","metadata":{},"source":"We then index the dataframe based on those rows. We get a dataframe that only has data from the store we want."}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::1481518f-71ce-4d23-94f0-1dd53e2d8aa9","text/plain":"smc-blob::ea3a5920-3b62-4c19-92d2-2a5fb5de2735"},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":"topstoredf = iowadfcl[subsetrows]\ntopstoredf.head()"}
{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"Store Number    821926.00\nSalesToStore    781295.08\ndtype: float64"},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":"iowadfcl[iowadfcl['Date']=='11/03/2016'].sum()"}
{"cell_type":"markdown","metadata":{},"source":"There is a lot of duplicated information here. We only really care about the SalesDate and the SalesToStore column. So we'll drop the other columns in this dataframe and assign it the same name (effectively dropping the columns)."}
{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"trusted":true},"outputs":[],"source":"topstoredf = topstoredf.drop(['Date', 'Store Number', 'City', 'Zip Code', 'County', 'Total Sales to Store (Dollars)'], axis=1)"}
{"cell_type":"markdown","metadata":{},"source":"We now want to sort the data by date so that it is listed in chronological order. We will use the `sort_values()` function (in place). We tell pandas which column to sort by."}
{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::42b8f6ed-549e-4c9b-b52f-4f7bee62b6ee","text/plain":"       SalesToStore  SalesDate\n2640       27592.98 2016-10-03\n12885       1621.20 2016-10-04\n1105       34386.10 2016-10-06\n13145      30216.88 2016-10-10\n1483       37319.85 2016-10-13"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":"topstoredf.sort_values('SalesDate',inplace=True)\ntopstoredf.head()"}
{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"SalesToStore                  27593\nSalesDate       2016-10-03 00:00:00\nName: 2640, dtype: object"},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":"topstoredf.loc[2640]"}
{"cell_type":"markdown","metadata":{},"source":"### Resetting the index\n\nWe haven't really paid much attention to the pandas index column up to this point, but now we'll take a look at it. The far left column of our new dataframe contains the indices for the dataframe- note that they are neither ordered nor sequential. After subsetting and re-ordering the dataset, they are a mess! We can fix this by resetting the index of the dataframe. This is helpful when we want to move forward with a subset of data. We'll pass two parameters to the `reset_index()` function. The first tells pandas to change our dataframe (instead of creating a copy), the second tells pandas that we don't care about the old index and so we should drop it."}
{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::860b5c61-e7f2-499a-be2e-f95b28110165","text/plain":"   SalesToStore  SalesDate\n0      27592.98 2016-10-03\n1       1621.20 2016-10-04\n2      34386.10 2016-10-06\n3      30216.88 2016-10-10\n4      37319.85 2016-10-13"},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":"topstoredf.reset_index(inplace=True,drop=True)\ntopstoredf.head()"}
{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"SalesToStore                  27593\nSalesDate       2016-10-03 00:00:00\nName: 0, dtype: object"},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":"topstoredf.loc[0]"}
{"cell_type":"markdown","metadata":{},"source":"Before we move on, let's look at this data. It is always a good idea to have some idea of what the data look like!"}
{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::82e8e3de-9bd7-4e1c-9997-eed415c40aed","text/plain":"<matplotlib.figure.Figure at 0x7f3d02d3a2e8>"},"metadata":{},"output_type":"display_data"}],"source":"import matplotlib.pyplot as plt\n# We will plot the data values and set the linestyle to 'None' which will not plot the line. We also want to show the individual data points, so we set the marker.\nplt.plot(topstoredf['SalesDate'].values, topstoredf['SalesToStore'].values, linestyle='None', marker='o')\n# autofmt_xdate() tells the computer that it should treat the x-values as dates and format them appropriately. This is a figure function, so we use gcf() to \"get current figure\"\nplt.gcf().autofmt_xdate()"}
{"cell_type":"markdown","metadata":{},"source":"### A Second dataset\n\nNow we need another dataset to work with. Let's try pulling the Dow Jones Industrial average index for each day. We'll download the data from [here](https://fred.stlouisfed.org/series/DJIA/downloaddata). We will also use a nice tool in the `read_csv()` function: it can parse a date column and put it in the appropriate datetime format all in one step."}
{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"DATE     datetime64[ns]\nVALUE           float64\ndtype: object\n"},{"data":{"text/html":"smc-blob::7c9072fe-20ea-4e5f-b74c-59f4d9e3007f","text/plain":"        DATE     VALUE\n0 2016-10-03  18253.85\n1 2016-10-04  18168.45\n2 2016-10-05  18281.03\n3 2016-10-06  18268.50\n4 2016-10-07  18240.49"},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":"djiadf = pd.read_csv('Class05_DJIA_data.csv',parse_dates=[0])\nprint(djiadf.dtypes)\ndjiadf.head()"}
{"cell_type":"code","execution_count":20,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"True"},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":"# We'll rename the second column to something a little more descriptive\ndjiadf.rename(columns={'VALUE':'DowJonesAvg'},inplace=True)\n#Let's check for problems:\ndjiadf.isnull().values.any()"}
{"cell_type":"code","execution_count":21,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DATE</th>\n      <th>DowJonesAvg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>38</th>\n      <td>2016-11-24</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>2016-12-26</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         DATE  DowJonesAvg\n38 2016-11-24          NaN\n60 2016-12-26          NaN"},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":"# Looks like there are two problem rows. Let's identify which rows by indexing the dataframe on the isnull() data\ndjiadf[djiadf['DowJonesAvg'].isnull()]"}
{"cell_type":"code","execution_count":22,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"False"},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":"# These were both holidays and shouldn't be included in the data set anyway! We can drop them in place, modifying our dataframe.\ndjiadf.dropna(inplace=True)\ndjiadf.isnull().values.any()"}
{"cell_type":"markdown","metadata":{},"source":"And, of course, let's plot it to see what it looks like."}
{"cell_type":"code","execution_count":23,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::542ace55-7805-4f2a-bd08-19d343cd79d7","text/plain":"<matplotlib.figure.Figure at 0x7f3ce3505470>"},"metadata":{},"output_type":"display_data"}],"source":"# We will plot the data values and set the linestyle to 'None' which will not plot the line. We also want to show the individual data points, so we set the marker.\nplt.plot(djiadf['DATE'].values, djiadf['DowJonesAvg'].values, linestyle='None', marker='o')\n# autofmt_xdate() tells the computer that it should treat the x-values as dates and format them appropriately. This is a figure function, so we use gcf() to \"get current figure\"\nplt.gcf().autofmt_xdate()"}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"## Joining Data\n\nWe now want to join the data from the store and the Dow Jones Industrial average. This will let us plot the two together to see if there is any correlation between the DJIA and the purchases that the store is making.\n\nIn order to join two dataframes we need to know how to align the rows or how to match up the data. The easiest way to do this is to try it out with an example to see how it works. We are going to follow [this tutorial](http://chrisalbon.com/python/pandas_join_merge_dataframe.html) on merging data in pandas. We start by creating two small test dataframes."}
{"cell_type":"code","execution_count":24,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::745d7c00-0763-4946-94d0-f75fa599ed0f","text/plain":"  patient_id first_name last_name  visit_number\n0          A       Alex  Anderson             1\n1          B        Amy  Ackerman             2\n2          C      Allen       Ali             3\n3          D      Alice      Aoni             4\n4          E     Ayoung   Atiches             5"},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":"raw_data = {\n        'patient_id': ['A', 'B', 'C', 'D', 'E'],\n        'first_name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], \n        'last_name': ['Anderson', 'Ackerman', 'Ali', 'Aoni', 'Atiches'],\n        'visit_number': [1,2,3,4,5],}\ndf_a = pd.DataFrame(raw_data, columns = ['patient_id', 'first_name', 'last_name','visit_number'])\ndf_a"}
{"cell_type":"code","execution_count":25,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::fbb90f75-79ce-459f-9096-c5b5d47a6392","text/plain":"  doctor_id first_name last_name  visit_number\n0         G      Billy    Bonder             4\n1         H      Brian     Black             5\n2         I       Bran   Balwner             6\n3         A      Bryce     Brice             7\n4         B      Betty    Btisan             8"},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":"raw_data = {\n        'doctor_id': ['G', 'H', 'I', 'A', 'B'],\n        'first_name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], \n        'last_name': ['Bonder', 'Black', 'Balwner', 'Brice', 'Btisan'],\n        'visit_number': [4,5,6,7,8]}\ndf_b = pd.DataFrame(raw_data, columns = ['doctor_id', 'first_name', 'last_name','visit_number'])\ndf_b"}
{"cell_type":"markdown","metadata":{},"source":"How do we tack `df_b` on the end of `df_a`? This is useful if you have pieces of a dataset that you want to combine together. Note that pandas doesn't, by default, reset the index for the new dataframe. We'd have to do that afterwards if we want to. Notice that there are now `NaN` values in the table! The first dataframe didn't have the `doctor_id` column, so pandas filled in those values with `NaN`. Same for the second dataframe and the `patient_id` values. We could drop those columns if we wanted to."}
{"cell_type":"code","execution_count":26,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::00510e59-c06b-4811-83e4-0b198b918ae8","text/plain":"smc-blob::09452775-4726-48ce-af54-a750f839cbf2"},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":"pd.concat([df_a, df_b])"}
{"cell_type":"markdown","metadata":{},"source":"What if we want to join together the rows that have the same visit_number? Looking at the table, we see that Billy Bonder and Alice Aoni have the same `visit_number`. Let's merge the two dataframes together to see how that works."}
{"cell_type":"code","execution_count":27,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::34601bd2-1c73-4cac-a59c-8799bea543e8","text/plain":"  patient_id first_name_x last_name_x  visit_number doctor_id first_name_y  \\\n0          D        Alice        Aoni             4         G        Billy   \n1          E       Ayoung     Atiches             5         H        Brian   \n\n  last_name_y  \n0      Bonder  \n1       Black  "},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":"pd.merge(df_a,df_b,on='visit_number')"}
{"cell_type":"markdown","metadata":{},"source":"So we now have a much smaller dataframe that only has the rows where `visit_number` was the same in both. Note that pandas also has changed the column names. This is because there were duplicate names and it can't have that to maintain unique addressing. Look at [the documentation for pandas](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) to see how to change the suffixes that pandas appends.\n\nWhat if, instead, we wanted to merge rows where the `patient_id` from one matches the `doctor_id` from the other? We can do that, too. The two dataframes are given the designations **left** and **right** for the first and second dataframes, respectively. So we need to identify the columns in each to match."}
{"cell_type":"code","execution_count":28,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::7e350830-d7b6-46d9-8ec5-b16bbefbb4a7","text/plain":"  patient_id first_name_x last_name_x  visit_number_x doctor_id first_name_y  \\\n0          A         Alex    Anderson               1         A        Bryce   \n1          B          Amy    Ackerman               2         B        Betty   \n\n  last_name_y  visit_number_y  \n0       Brice               7  \n1      Btisan               8  "},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":"pd.merge(df_a,df_b,left_on='patient_id',right_on='doctor_id')"}
{"cell_type":"markdown","metadata":{},"source":"What if we want to keep *all* of the entries in the left dataframe and only add in values on the right where they match? We can do that, too!"}
{"cell_type":"code","execution_count":29,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::2f734d10-5ab6-4f27-bc69-f13586464a68","text/plain":"smc-blob::33241085-e97d-4314-922d-645834ae589c"},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":"pd.merge(df_a,df_b,on='visit_number', how='left')"}
{"cell_type":"markdown","metadata":{},"source":"Not surprisingly, we can do the right-side, too."}
{"cell_type":"code","execution_count":30,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::09872bf3-a15e-42b7-861b-e4ede60cc6a7","text/plain":"smc-blob::2affe864-71d8-4e25-9d23-7d1e8dfd6966"},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":"pd.merge(df_a,df_b,on='visit_number', how='right')"}
{"cell_type":"markdown","metadata":{},"source":"Finally, we can do both and include all the data from both dataframes, matching where possible. That is called an \"outer\" join."}
{"cell_type":"code","execution_count":31,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::cce727f0-b2ba-4765-8fc0-3ab8c271224d","text/plain":"smc-blob::af029eba-4cfc-454b-9b04-ff30167ec42f"},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":"pd.merge(df_a,df_b,on='visit_number', how='outer')"}
{"cell_type":"markdown","metadata":{},"source":"#### Choosing the Merge Index\n\nHow do you choose the merge index? We used both the `visit_number` and the `patient/doctor_id` values. The key is to be able to merge on some common feature. This may be date (like we show below), but it could also be on zip code, a city name, the month number, financial quarter, or any other piece of information that you have common between your two datasets. You need to make sure that the merge indices have the same data type and are in the same format. For example, the computer doesn't know that 'DALLAS' and 'Dallas' are the same city. We can use the tools we've previously covered to clean up the data prior to attempting the merge."}
{"cell_type":"markdown","metadata":{},"source":"## Merging the Store and DJIA data\n\nSince we want to keep all of the store data and merge in the DJIA data where we can, we are going to try a left join. We are also going to try joining on the date - they should match up reasonably well."}
{"cell_type":"code","execution_count":32,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::eb9f1f71-c2a3-43ca-987e-1a3050f5c773","text/plain":"   SalesToStore  SalesDate       DATE  DowJonesAvg\n0      27592.98 2016-10-03 2016-10-03     18253.85\n1       1621.20 2016-10-04 2016-10-04     18168.45\n2      34386.10 2016-10-06 2016-10-06     18268.50\n3      30216.88 2016-10-10 2016-10-10     18329.04\n4      37319.85 2016-10-13 2016-10-13     18098.94"},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":"storeDJ = pd.merge(topstoredf, djiadf, left_on='SalesDate',right_on='DATE',how='left')\nstoreDJ.head()"}
{"cell_type":"markdown","metadata":{},"source":"Now we can do what we wanted to do: plot the data to see if there is a correlation here!"}
{"cell_type":"code","execution_count":33,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f3ceaa7c358>"},"execution_count":33,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::a6e3899b-fbba-4e5b-94e8-4180415fddc5","text/plain":"<matplotlib.figure.Figure at 0x7f3ceaa6a160>"},"metadata":{},"output_type":"display_data"}],"source":"storeDJ.plot.scatter(x='DowJonesAvg',y='SalesToStore')"}
{"cell_type":"markdown","metadata":{},"source":"Well, after all that work, it doesn't look like there is anything there... Perhaps that isn't too surprising as the purchases by a single store may not depend on a macro-scale indicator like the Dow Jones Industrial average. But we didn't know that until we checked.\n\n\n## In-class Activity\n\nInstead of looking at the sales to one store, what if we look at the total sales to all of the stores in the state of Iowa? Go through the process of grouping that data and joining it with the DJIA data. Plot the results and show me before you leave."}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"# Assignment\n\nYour assignment this week is to do data enriching on your dataset. You can either add/join data or do some kind of grouping to subset the data. I want documentation of what you did and how it turned out. Like our example today, you may find that the data enriching doesn't do much.\n"}
{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":""}