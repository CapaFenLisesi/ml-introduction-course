{"kernelspec":{"display_name":"Python 3 (Anaconda)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 09\n## ML Models: Clustering\n\nWe continue working with unsupervised learning in this class. This time we are interested in separating out groups of data or creating *clusters*. This can be useful if we are looking for patterns in our data. One pattern could be that the data clumps together around certain points. However, before we can check to see if there are data clusters, we have to know how many cluster points to look for. Fortunately the **K-means Classifier** algorithm works very quickly, so we should be able to try a variety of cluster numbers fairly quickly.\n\n### Demo\n\nBefore we dive into working with our own data, there is an [excellent visualization tool](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) that shows how this algorithm works. We will explore this together as a class before we move on to the next step.\n\n### Sample Data\n\nI am going to follow [a tutorial](https://www.datascience.com/blog/introduction-to-k-means-clustering-algorithm-learn-data-science-tutorials) that does a good job of describing how k-means clustering works. The data are based on measurements of truck drivers. There are two features: the mean percentage of time a driver was >5 mph over the speed limit and the mean distance driven per day.\n\nWe'll use a [scikit tool](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster) to try the unsupervised clustering on the data. Naturally we'll start by loading the sample data and plotting it to see what we've got."}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::c3b03c1d-456b-4097-92f6-0bb2a3f2f793","text/plain":"            Distance_Feature  Speeding_Feature\nDriver_ID                                     \n3423311935             71.24                28\n3423313212             52.53                25\n3423313724             64.54                27\n3423311373             55.69                22\n3423310999             54.58                25"},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":"import pandas as pd\nimport matplotlib.pyplot as plt\ndf1=pd.read_csv('Class09_cluster_example1.csv',index_col=0)\ndf1.head()\n"}
{"cell_type":"markdown","metadata":{},"source":"Not knowing exactly what we're working with, let's get the minima and maxima of the two features. We'll use this later to create plots of our predictions."}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"(-1, 101)"},"execution_count":2,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::d4505b41-117a-4ab9-88d7-a24a300dc702","text/plain":"<matplotlib.figure.Figure at 0x7fb2cd892780>"},"metadata":{},"output_type":"display_data"}],"source":"df1.plot.scatter(x='Distance_Feature',y='Speeding_Feature',marker='.')\nx_min, x_max = df1['Distance_Feature'].min() - 1, df1['Distance_Feature'].max() + 1\ny_min, y_max = df1['Speeding_Feature'].min() - 1, df1['Speeding_Feature'].max() + 1\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)"}
{"cell_type":"markdown","metadata":{},"source":"There are a couple of ways we could try splitting up this data. Like we saw in the demonstration, we have to choose the number of clusters ($k$) before we start. We'll try a couple of values and then look at them to see how they map against the data. There isn't any point in splitting the data into training/testing subsets because we don't have a label to train on. So we'll fit all the data."}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"from sklearn.cluster import KMeans\n\nkmeans2 = KMeans(n_clusters=2).fit(df1)"}
{"cell_type":"markdown","metadata":{},"source":"We will make a visualization like we've done with the classification algorithms: we want to map out which regions will be predicted to be which class. That will take a bit of work."}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7fb2b7345b70>"},"execution_count":4,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::300c456e-cdc9-4533-8396-2a7bfd319423","text/plain":"<matplotlib.figure.Figure at 0x7fb2cf058908>"},"metadata":{},"output_type":"display_data"}],"source":"import numpy as np\n\n# Step size of the mesh. Decrease to increase the quality of the plot.\nh = 0.5     # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n# Create the mesh for plotting the decision boundary\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n# Get the centroid (or center position) of each region so we can plot that, too.\ncentroids = kmeans2.cluster_centers_\n\n# First plot the mesh that has the predictions for each point on the mesh.\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\n\n# Now put the points on top\nplt.plot(df1['Distance_Feature'], df1['Speeding_Feature'], 'k.', markersize=2)\n\n# Plot the centroids as a white X\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='w', zorder=10)\nplt.title('K-means with 2 Clusters')\n\n# And fix the plot limits and labels.\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.ylabel('Speeding_Feature')\nplt.xlabel('Distance_Feature')\n"}
{"cell_type":"markdown","metadata":{},"source":"So this looks pretty good. There is a nice clear boundary between the two halves of the plot. The centroids (marked as white \"X\"s on the plot) look about right, too. I'm happy with this clustering. \n\nIf we get a future point in now, we can easily classify it as belonging to one of these two groups. For example, we could now create a feature based on this grouping and then use that for other machine learning. \n\n## Cluster-data mismatches\n\nNow let's see what happens if we pick 3 clusters from the start."}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7fb2b7361c88>"},"execution_count":5,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::1dda6b8d-24e3-47b3-990b-512366fc6785","text/plain":"<matplotlib.figure.Figure at 0x7fb2ce153e48>"},"metadata":{},"output_type":"display_data"}],"source":"kmeans3 = KMeans(n_clusters=3).fit(df1)\n\n\nZ = kmeans3.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\ncentroids = kmeans3.cluster_centers_\n\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\n\nplt.plot(df1['Distance_Feature'], df1['Speeding_Feature'], 'k.', markersize=2)\n# Plot the centroids as a white X\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='w', zorder=10)\nplt.title('K-means with 3 Clusters')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.ylabel('Speeding_Feature')\nplt.xlabel('Distance_Feature')"}
{"cell_type":"markdown","metadata":{},"source":"The algorithm does the best it can with what we gave it- it found three clusters of data. But the decision boundaries do not match the data very well. So let's try 4 instead."}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7fb2b722e5f8>"},"execution_count":6,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::75d98c84-6ffe-4208-8b91-0cfd11b4c51e","text/plain":"<matplotlib.figure.Figure at 0x7fb2cf058a90>"},"metadata":{},"output_type":"display_data"}],"source":"kmeans4 = KMeans(n_clusters=4).fit(df1)\n\nZ = kmeans4.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\ncentroids = kmeans4.cluster_centers_\n\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\n\nplt.plot(df1['Distance_Feature'], df1['Speeding_Feature'], 'k.', markersize=2)\n# Plot the centroids as a white X\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='w', zorder=10)\nplt.title('K-means with 4 Clusters')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.ylabel('Speeding_Feature')\nplt.xlabel('Distance_Feature')"}
{"cell_type":"markdown","metadata":{},"source":"That looks better. There are at least three good groups but the algorithm picks out the upper right-hand corner as the fourth grouping. I think we can work with this.\n\n\n## Using k-means groups as features\n\nWhat if we want to add the groups to the dataframe to use it for other machine learning algorithms? We'll add in the feature column then plot the data using this new feature.\n"}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7fb2b7312b70>"},"execution_count":7,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::50ee74c0-c8b3-462f-82af-121b12b1c2e0","text/plain":"<matplotlib.figure.Figure at 0x7fb2cd8926a0>"},"metadata":{},"output_type":"display_data"}],"source":"# Create the new column based on the labels from the kmeans fit\ndf1['kmeansgroup'] = kmeans4.labels_\n\n# We group the data by this column\ngroups = df1.groupby('kmeansgroup')\n\n\n# Then plot it\ntrainfig, ax = plt.subplots()\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n# The next step is to cycle through the groups (based on our categories) and plot each one on the same axis.\nfor name, group in groups:\n    ax.plot(group['Distance_Feature'], group['Speeding_Feature'], marker='o', linestyle='', ms=6, label=name)\n    ax.set_aspect(1)\nax.legend(bbox_to_anchor=(1.2,0.5))\nax.set_xlabel('Distance_Feature')\nax.set_ylabel('Speeding_Feature')"}
{"cell_type":"markdown","metadata":{},"source":"We didn't change the names of the features- the clusters are named 0-3, but that's probably good enough for now.\n\n## Working with geographical clusters\n\nWe'll now move to working with real data. This data comes from the [US Geological Service](https://earthquake.usgs.gov/earthquakes/search/). I searched for all of the magnitude 1.0 and greater earthquakes for the state of Indiana from the last 40 years. We'll do a little data exploration on this dataset to see what we've got."}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"time               datetime64[ns]\nlatitude                  float64\nlongitude                 float64\ndepth                     float64\nmag                       float64\nmagType                    object\nnst                       float64\ngap                       float64\ndmin                      float64\nrms                       float64\nnet                        object\nid                         object\nupdated                    object\nplace                      object\ntype                       object\nhorizontalError           float64\ndepthError                float64\nmagError                  float64\nmagNst                    float64\nstatus                     object\nlocationSource             object\nmagSource                  object\ndtype: object\n"},{"data":{"text/html":"smc-blob::dd979a7b-f981-4669-972c-657aebd04245","text/plain":"smc-blob::3db977ad-78f1-41c1-bf32-236f4ef5efc7"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":"eqINdf = pd.read_csv('Class09_USGS_IN_data.csv',parse_dates=[0])\nprint(eqINdf.dtypes)\neqINdf.head()"}
{"cell_type":"markdown","metadata":{},"source":"Let's look first at the earthquake magnitude vs time. It looks like there are a bunch of other columns we aren't going to need.\n"}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7fb2b7084710>"},"execution_count":9,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::a44ff20b-b9fe-4b11-8f2c-7792e26bcee6","text/plain":"<matplotlib.figure.Figure at 0x7fb2b7211c50>"},"metadata":{},"output_type":"display_data"}],"source":"# I want to make the plots appear bigger. This is how to do that.\nplt.rcParams['figure.figsize'] = (12,8)\n\nplt.plot(eqINdf['time'],eqINdf['mag'])\nplt.xlabel('time')\nplt.ylabel('Magnitude')"}
{"cell_type":"markdown","metadata":{},"source":"That's not really what I want. What I want is the frequency of earthquakes over time. How about plotting a histogram of the number of earthquakes of each magnitude over the time window. We'll need to create a feature where we round the magnitude first. Then we can use a seaborn function to plot the different magnitudes."}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<seaborn.axisgrid.FacetGrid at 0x7fb2e060eda0>"},"execution_count":10,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::7244c4f6-532c-40fc-b5c5-060df90a1787","text/plain":"<matplotlib.figure.Figure at 0x7fb2b7096cc0>"},"metadata":{},"output_type":"display_data"}],"source":"import seaborn as sns\neqINdf['maggroup'] = eqINdf['mag'].apply(np.round)\ng= sns.FacetGrid(eqINdf, col=\"maggroup\",sharey=False)\ng = g.map(plt.hist, \"time\",bins=100)\ng.set_xticklabels(rotation=45)"}
{"cell_type":"markdown","metadata":{},"source":"That looks more like what I want. The earthquakes look like that happen fairly regularly and mostly randomly.\n\nLet's now look at the geographical data. I want to do a couple of things here. First, I want to plot the map so that I know where the earthquakes happen. We'll use the [Basemap library](http://matplotlib.org/basemap/) to do that part. \n\nI also want to plot the size of the points as proportional to the magnitude of the earthquake. The way I'm going to do this is to adjust the point size so that the `s` input to the `scatter()` function is equal to:\n\n$size = 3^{magnitude}$.\n\nThat will make the plot look good.\n\nFinally, I want to color the points chronologically so that, as we go through time, the colors change based on the colors of a rainbow. To do that, I'm going to need a `days since start` feature. Let's do both of those first.\n"}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"markersize = 3**(eqINdf['mag'].values)\n\n# Get the first date in the time series\ndate0 = eqINdf['time'][0]\n\n# Subtract the first date from each date and convert it to the number of days\neqINdf['dayssincestart'] = eqINdf['time'].apply(lambda x: (x-date0).days)\n\ncolors = eqINdf['dayssincestart']"}
{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:3289: MatplotlibDeprecationWarning: The ishold function was deprecated in version 2.0.\n  b = ax.ishold()\n/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:3298: MatplotlibDeprecationWarning: axes.hold is deprecated.\n    See the API Changes document (http://matplotlib.org/api/api_changes.html)\n    for more details.\n  ax.hold(b)\n"},{"data":{"text/plain":"[<matplotlib.text.Text at 0x7fb2ab2f7a90>,\n <matplotlib.text.Text at 0x7fb2ab327668>,\n <matplotlib.text.Text at 0x7fb2a9ae1898>,\n <matplotlib.text.Text at 0x7fb2a9ae2518>,\n <matplotlib.text.Text at 0x7fb2a9ae5160>]"},"execution_count":12,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::c557bf60-67fd-4652-b197-89ab2fbf9472","text/plain":"<matplotlib.figure.Figure at 0x7fb2ab964518>"},"metadata":{},"output_type":"display_data"}],"source":"from mpl_toolkits.basemap import Basemap\nfrom matplotlib.patches import Polygon\n\n# Draw the map based on the maximum and minimum positions of our data points\nm = Basemap(llcrnrlon=eqINdf['longitude'].min()*1.01,llcrnrlat=eqINdf['latitude'].min()*0.99,urcrnrlon=eqINdf['longitude'].max()*0.99,urcrnrlat=eqINdf['latitude'].max()*1.01,\n        projection='merc')\n\n# Add the state boundaries to the map\nm.readshapefile('st99_d00', name='states', drawbounds=True)\n\n# Prep the data for plotting on the map\nx,y = m(eqINdf['longitude'].values, eqINdf['latitude'].values)\n\n\n# Plot the data points on the map\nm.scatter(x,y, s=markersize, c=colors, marker=\"o\",cmap=plt.cm.rainbow,alpha=0.7)\n\n# Now set up the color bar with ticks every 10 years\ncbar = plt.colorbar(ticks=np.arange(0,colors.max(),3650))\ncbar.ax.set_yticklabels(np.arange(1976,2017,10))  # vertically oriented colorbar"}
{"cell_type":"markdown","metadata":{},"source":"Now I want to try clustering the data. It looks like there may be 5-6 clusters here. We'll try one to see."}
{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:3413: MatplotlibDeprecationWarning: The ishold function was deprecated in version 2.0.\n  b = ax.ishold()\n/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:3452: MatplotlibDeprecationWarning: axes.hold is deprecated.\n    See the API Changes document (http://matplotlib.org/api/api_changes.html)\n    for more details.\n  ax.hold(b)\n/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:3289: MatplotlibDeprecationWarning: The ishold function was deprecated in version 2.0.\n  b = ax.ishold()\n/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:3298: MatplotlibDeprecationWarning: axes.hold is deprecated.\n    See the API Changes document (http://matplotlib.org/api/api_changes.html)\n    for more details.\n  ax.hold(b)\n"},{"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7fb2a9b41278>"},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::2d0814a5-4591-4f5a-9609-bc362da59e81","text/plain":"<matplotlib.figure.Figure at 0x7fb2aba1a978>"},"metadata":{},"output_type":"display_data"}],"source":"kmeansIN1 = KMeans(n_clusters=6,n_init=30,max_iter=1000).fit(eqINdf[['latitude','longitude']])\n\n# Obtain labels for each point in mesh. Use last trained model.\nh=0.05\nxx, yy = np.meshgrid(np.arange(eqINdf['latitude'].min()*0.99, eqINdf['latitude'].max()*1.01, h), np.arange(eqINdf['longitude'].min()*1.01, eqINdf['longitude'].max()*0.99, h))\n\nZ = kmeansIN1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n# Get the centroid of each region\ncentroids = kmeansIN1.cluster_centers_\n\n# Draw the map based on the maximum and minimum positions of our data points\nm = Basemap(llcrnrlon=eqINdf['longitude'].min()*1.01,llcrnrlat=eqINdf['latitude'].min()*0.99,urcrnrlon=eqINdf['longitude'].max()*0.99,urcrnrlat=eqINdf['latitude'].max()*1.01,\n        projection='merc')\n\n# Add the state boundaries to the map\nm.readshapefile('st99_d00', name='states', drawbounds=True)\n\n# Prep our cluster boundaries for plotting on the map\nxb,yb = m(yy,xx)\n\n# Plot the boundaries\nm.pcolor(xb,yb,Z,cmap=plt.cm.Paired)\n\n# Plot the points - all the same color, but still sizing them by magnitude\nm.scatter(x,y, s=markersize, marker=\"o\",alpha=0.2)\n\n# Plot the centroids as a white X\nxc,yc = m(centroids[:, 1], centroids[:, 0])\nm.scatter(xc,yc,marker='x', s=169, linewidths=3,color='w', zorder=10)"}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"That looks about right - the white X marks may correspond to the central locations of the faults. That is something to look into.\n\n### In-class Activity\n\nI've got another USGS dataset. This time it maps the state of Oklahoma during the same time period. Take a look at the data to see what we've got (`Class09_USGS_OK_data.csv`).\n\n### Homework\n\nTake a look at your own data and see if there are clusters you could identify. If not, pick out a different dataset (like a crime map or traffic map) and do cluster analysis on it. The homework will be due next Tuesday."}