{"kernelspec":{"display_name":"Python 2 (SageMath)","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 13\n## ML Models: Neural Networks\n\nOur final topic this course will be to introduce neural networks. As before we won't go into the algorithms or how they work, but rather we will spend the time talking about their key features and when to use them. If you want more details about how they work, I recommend [this simple tutorial](http://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html). We will be following the work of [this tutorial](http://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/) in the order and tools we use for these notes.\n\n### Nerual Network Basics\n\nThe basic idea is that, instead of using a single Perceptron or Logistic Regression classifier, what if we used **many** of them? But we only want a single output, so maybe it makes more sense to build up an intermediate set of layers where each node of the network takes in all of the previous layer's outputs as inputs. Then, the final layer will have a single output that gives us our final prediction.\n\nThis may sound crazy, but there is a biological basis for the model: how brains work in organisms. The neurons behave in a similar fashion, thus giving rise to the name \"neural\" network. The key components to a neural network are the number of layers and how they are connected to each other.\n\n### Image Recognition\n\nWe'll use the MNIST handwriting dataset for our neural network tests. We've seen this data a couple of times but previously we only used a smaller subset of the total data. We're going to need the entire dataset this time. To save disk space, I've used the python pickle package and the gzip package to store it in a compressed form.\n\nAlso, we'll have to use a couple of other new tools today: the neural network packages we have access to on SageMath don't work well with Pandas dataframes. So we'll do some of the work today using numpy tools.\n\nWe've already split the data into training/testing subgroups and stored it that way in the file. We'll load in the file and set up our features and labels."}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Training Features:\n(60000, 28, 28)\nTraining Labels:\n(60000,)\nTesting Features:\n(10000, 28, 28)\nTesting Labels:\n(10000,)\n"}],"source":"import numpy as np\n# fix random seed for reproducibility\nnp.random.seed(23)\n\n# load data\ndef load_data(path='Class13_mnist.pkl.gz'):\n    import gzip\n    from six.moves import cPickle\n    import sys\n    #path = get_file(path, origin='https://s3.amazonaws.com/img-datasets/mnist.pkl.gz')\n\n    if path.endswith('.gz'):\n        f = gzip.open(path, 'rb')\n    else:\n        f = open(path, 'rb')\n\n    if sys.version_info < (3,):\n        data = cPickle.load(f)\n    else:\n        data = cPickle.load(f, encoding='bytes')\n\n    f.close()\n    return data  # (X_train, y_train), (X_test, y_test)\n\n(features_train, labels_train), (features_test, labels_test) = load_data('Class13_mnist.pkl.gz')\n\nprint('Training Features:')\nprint(features_train.shape)\nprint('Training Labels:')\nprint(labels_train.shape)\nprint('Testing Features:')\nprint(features_test.shape)\nprint('Testing Labels:')\nprint(labels_test.shape)"}
{"cell_type":"markdown","metadata":{},"source":"So we now have 60,000 training images and 10,000 test images. Each image is $28 \\times 28$ pixels and has an output label indicating what number is supposed to have been drawn.\n\nWe'll take a quick look at the data to make sure it makes sense. Visualizing is usually our best option."}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Expected Digit: 1\nMax value in image: 255\n"},{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f1d5950dd10>"},"execution_count":2,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"<matplotlib.figure.Figure at 0x7f1d597f8f90>"},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"smc-blob::8abc0ac3-e626-4f28-8999-1bcee9ef9d12","text/plain":"<matplotlib.figure.Figure at 0x7f1d597f8450>"},"metadata":{},"output_type":"display_data"}],"source":"import matplotlib.pyplot as plt\ntestnum = 45076\ntestimage = features_train[testnum]\nprint('Expected Digit: {0:d}'.format(labels_train[testnum]))\nprint('Max value in image: {}'.format(testimage.max()))\nplt.gray() \nplt.matshow(testimage) "}
{"cell_type":"markdown","metadata":{},"source":"The neural network will work much better if we normalize the inputs. It looks like the image values run from 0 to 255. Let's re-scale that to 0-1. We change the type to a float32 since the keras tool uses that internally.\n\nWe'll also get the number of pixels in each image- we'll need that later. Finally we need to add in another dimension - this sounds weird, but it required for the neural network to use the image data. This is pretty typical of working with image learning using neural networks. This dimension represents the number of color channels in the image. For example, our gray-scale images have (1,28,28). A full-color RGB image would have the dimensions (3,28,28).\n"}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"trusted":true},"outputs":[],"source":"# normalize inputs from 0-255 to 0-1\nfeatures_train_set = features_train.astype('float32') / 255\nfeatures_test_set = features_test.astype('float32') / 255\npixdim1, pixdim2 = features_train_set.shape[1], features_train_set.shape[2]\nnum_pixels = pixdim1*pixdim2\n\nfeatures_train_set = features_train_set.reshape(features_train_set.shape[0], 1, pixdim1, pixdim2)\nfeatures_test_set = features_test_set.reshape(features_test_set.shape[0], 1, pixdim1, pixdim2)"}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"(60000, 1, 28, 28)"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"features_train_set.shape"}
{"cell_type":"markdown","metadata":{},"source":"Finally, we need to shift our output predictions to a dummy variable since we are looking for a categorical output. Whether this step is necessary is dependent on the specific neural network tool you are using: some don't require this, others do. Because the Keras tool we will be using requires that classification models be set up this way, we'll do it. There is a Keras tool to make this easy. We'll also get the number of classes in the output test dataset."}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"Using TensorFlow backend.\n"}],"source":"from keras import backend as K\nK.set_image_dim_ordering('th')\nfrom keras.utils import np_utils\n\n# one hot encode outputs\nlabels_train_cat = np_utils.to_categorical(labels_train)\nlabels_test_cat = np_utils.to_categorical(labels_test)\nnum_classes = labels_test_cat.shape[1]"}
{"cell_type":"markdown","metadata":{},"source":"## Simple One-Layer Network\n\nWe're now ready to build our first neural network. This typically requires a couple of steps:\n\n1. Design the neural network's layers\n2. Train the network\n\nThe specific code you use to design the layers depends on the neural network tool you are using. The basic types of layers are the same for all of the tools, though. In this case we will do the following."}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"trusted":true},"outputs":[],"source":"# One Dense Layer model\n#-------------------------------------------------------------------------------\ndef simpleModel():\n    \n    from keras.layers.core import Activation, Dense\n    from keras.models import Sequential\n    \n    \n    # create model and tell the tool that each layer we define will be added in sequential order\n    model = Sequential()\n       \n    # our first layer is called a \"Dense\" layer - it is a fully connected set of nodes (every input connects to every node)\n    # We tell it what shape to expect the input: in this case it is the total number of input pixels.\n    # The output dimension is the number of nodes to create for this layer. We want one node for each pixel, so we use the number of pixels as our output dimension\n    model.add(Dense(input_dim=num_pixels, output_dim=num_pixels, init='normal'))\n    \n    # The next thing we need to do is tell the network how to adjust the probabilities that the nodes compute. We'll use the hyperbolic tangent:\n    # It is like the logistic regression, but on the scale of -1 to 1 instead of 0 to 1.\n    model.add(Activation('tanh'))\n    # Finally we need to create out output layer. In this tool we need to specify an output dimension the same as the number of classes we are trying to predict\n    model.add(Dense(output_dim=num_classes, activation='softmax'))\n    # This time, the softmax activation is the same as the logistic regression, but generalized for more than 2 output choices (we have 10 classes here)\n    #model.add(Activation('softmax'))\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model"}
{"cell_type":"markdown","metadata":{},"source":"The last thing we need to do is to flatten out our input arrays. This simple neural network works better if we are only working with a single dimension of data.\n\nThen we are ready now to train our model. We'll do a couple of things all at the same time: first we'll give the `fit()` function the test data for validation. It will use that data at every step to figure out how it is doing in terms of it's fit.\n\nWe'll also set up a couple of other parameters:\n* Number of Epochs: how many times should we repeat the network training, learning from each previous iteration\n* How large should we make our batches: how many images do we pile on at one time while training - typically about 1/100 of the total samples works well (as a general rule of thumb)."}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train on 60000 samples, validate on 10000 samples\nEpoch 1/2\n60000/60000 [==============================] - 16s - loss: 0.4903 - acc: 0.8512 - val_loss: 0.2815 - val_acc: 0.9184\nEpoch 2/2\n60000/60000 [==============================] - 16s - loss: 0.2599 - acc: 0.9239 - val_loss: 0.2341 - val_acc: 0.9299\nBaseline Accuracy 92.99%\n"}],"source":"# flatten 28*28 images to a 784 vector for each image\nX_train = features_train.reshape(features_train_set.shape[0], num_pixels)\nX_test = features_test.reshape(features_test_set.shape[0], num_pixels)\n\n# build the model\nmodel = simpleModel()\n# Fit the model\nmodel.fit(X_train, labels_train_cat, validation_data=(X_test, labels_test_cat), nb_epoch=2, batch_size=600, verbose=1)\n# Final evaluation of the model\nscores = model.evaluate(X_test, labels_test_cat, verbose=0)\nprint(\"Baseline Accuracy {:.2f}%\".format(scores[1]*100))"}
{"cell_type":"markdown","metadata":{},"source":"So we're above 90% accuracy after only 2 epochs. Try increasing the number of epochs to see if there is a point where this levels out.\n\nLet's look at the confusion matrix to see if there are any particularly problematic numbers."}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":" 9920/10000 [============================>.] - ETA: 0s"}],"source":"actuals = labels_test\npredictions = model.predict_classes(X_test)"}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy Score: 0.9299\n"},{"data":{"image/png":"smc-blob::cce68739-a9dc-41f6-ab89-dfcbba3f879b","text/plain":"<matplotlib.figure.Figure at 0x7f1d4bd91750>"},"metadata":{},"output_type":"display_data"}],"source":"import sklearn.metrics as metrics\n\ncnf_matrix = metrics.confusion_matrix(actuals, predictions)\n\ndef show_confusion_matrix(cnf_matrix, class_labels):\n    fig, ax = plt.subplots(figsize=(20, 10))\n    ax.matshow(cnf_matrix,cmap=plt.cm.YlGn,alpha=0.7)\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks(range(0,len(class_labels)))\n    ax.set_xticklabels(class_labels)\n    ax.set_ylabel('Actual Label', fontsize=16, rotation=90)\n    ax.set_yticks(range(0,len(class_labels)))\n    ax.set_yticklabels(class_labels)\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n\n    for row in range(len(cnf_matrix)):\n        for col in range(len(cnf_matrix[row])):\n            ax.text(col, row, cnf_matrix[row][col], va='center', ha='center', fontsize=16)\n        \nshow_confusion_matrix(cnf_matrix,class_labels=range(10))\nscore = metrics.accuracy_score(actuals, predictions)\nprint(\"Accuracy Score: {}\".format(score))"}
{"cell_type":"markdown","metadata":{},"source":"### Convolution Neural Networks\n\nOne of the things about image recognition is that there is often a lot of information about the image hiding in its edges, shapes, and patterns. If we only look at the image pixels, we miss using these other types of information. A simple way to train a model using more information is to use a trick called *convolution*. This essentially gives the model additional features to look at. We build a new neural network using both fully connected layers as well as convolution layers. \n\nA lot of work has been done by researchers to find sets of neural network layers that work well for different types of images. The following set works fairly well for the handwriting images or other simple images with patterns."}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"# Simplified LeNet model\n#-------------------------------------------------------------------------------\ndef leNet_model(pixdim1, pixdim2):\n    \n    from keras.layers.core import Activation, Dense\n    from keras.models import Sequential\n    \n    from keras.layers.convolutional import Convolution2D, MaxPooling2D\n    from keras.layers import Dropout, Flatten\n    \n    # create model\n    model = Sequential()\n    model.add(Convolution2D(20, 5, 5, border_mode='valid', input_shape=(1, pixdim1, pixdim2)))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Convolution2D(50, 3, 3))\n    model.add(Activation('tanh'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    \n    model.add(Dense(500))\n    model.add(Activation('tanh'))\n    model.add(Dense(40))\n    \n    model.add(Dense(num_classes, activation='softmax'))\n    # Compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n"}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Train on 60000 samples, validate on 10000 samples\nEpoch 1/2\n60000/60000 [==============================] - 509s - loss: 0.3385 - acc: 0.9021 - val_loss: 0.0886 - val_acc: 0.9718\nEpoch 2/2\n60000/60000 [==============================] - 462s - loss: 0.0780 - acc: 0.9772 - val_loss: 0.0593 - val_acc: 0.9810\nBaseline Accuracy 98.10%\n"}],"source":"# build the model\nmodel = leNet_model(pixdim1,pixdim2)\n# Fit the model\nmodel.fit(features_train_set, labels_train_cat, validation_data=(features_test_set, labels_test_cat), nb_epoch=2, batch_size=600, verbose=1)\n# Final evaluation of the model\nscores = model.evaluate(features_test_set, labels_test_cat, verbose=0)\nprint(\"Baseline Accuracy {:.2f}%\".format(scores[1]*100))"}
{"cell_type":"markdown","metadata":{},"source":"Even though this model takes significantly longer, it performs better than our simple single-layer model. This is even more the case when the images are more complicated than identifying handwritten digits.\n\n# Assignment\n\nYour assignment this week is to finish up your final project. I will collect a draft of you project via Canvas, due at the start of the final class period. I will split up the class period into blocks to spend with each student going over their project."}
{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":""}