{"kernelspec":{"display_name":"Python 3 (Anaconda)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 12\n## ML Models: Logistic Regression\n\nToday we are going to move to learning with image data. We will focus on the classification problem: can we train a model to identify an image? We will start with a simple model that will lead us to one of the best image recognition models so far: deep learning neural networks.\n\n### Classification Review\n\nRecall from previous classes that the goal in a classification problem is to predict one of a finite number of output classes. We've used the \"fast/slow\" data several times before and we'll review the simplist classifier algorithm that we first saw in Class04: the perceptron.\n\nAs usual, we load the data, do our train/test split, then train the model. We then plot the test data and the decision boundary."}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7fbb40d65f28>"},"execution_count":1,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::a5319ec9-499b-458a-898c-843bbfa309ff","text/plain":"<matplotlib.figure.Figure at 0x7fbb74cafbe0>"},"metadata":{},"output_type":"display_data"}],"source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import Perceptron, LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\n\n#Returning to Class04 data\nspeeddf = pd.read_csv(\"../Class04/Class04_speed_data.csv\",dtype={'Speed':'category'})\n\n#We'll use a different tool to plot the data now that we know how to group the data by a category. This will help us make better combined plots later on.\ngroups = speeddf.groupby('Speed')\n\n# Plot the data\ntrainfig, ax = plt.subplots()\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n# The next step is to cycle through the groups (based on our categories) and plot each one on the same axis.\nfor name, group in groups:\n    ax.plot(group['Grade'], group['Bumpiness'], marker='o', linestyle='', ms=4, label=name)\n    ax.set_aspect(1)\nax.legend(bbox_to_anchor=(1.0,0.5))\nax.set_xlabel('Grade')\nax.set_ylabel('Bumpiness')"}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy Score: 0.9\nMatthews Correlation Coefficient (MCC): 0.8148487556741162\n"},{"data":{"image/png":"smc-blob::c1db6cbd-c67c-43a5-9f82-de125b5e5eaf","text/plain":"<matplotlib.figure.Figure at 0x7fbb7d446438>"},"metadata":{},"output_type":"display_data"}],"source":"# Split the data into training and testing sets and prepare the features and labels\ntrain, test = train_test_split(speeddf, test_size=0.2, random_state=23)\n\nfeatures_train = train[['Grade','Bumpiness']].values\nlabels_train = train['Speed'].values\n\nfeatures_test = test[['Grade','Bumpiness']].values\nlabels_test = test['Speed'].values\n\nclass_labels = [\"slow\", \"fast\"]\n\n# Load the model, fit the data, get the predicted values (for scoring)\nmodel = Perceptron()\nmodel.fit(features_train,labels_train)\ny_pred = model.predict(features_test)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh\nx_min = 0.0; x_max = 1.0 # Mesh x size\ny_min = 0.0; y_max = 1.0  # Mesh y size\nh = .01  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Predict the boundary\nZ = pd.Series(model.predict(np.c_[xx.ravel(), yy.ravel()]), dtype='category').cat.codes.values.reshape(xx.shape)\n\n# First plot our points\ntestfig1, ax = plt.subplots()\nplt.pcolormesh(xx, yy, Z, cmap= plt.cm.cool, alpha=0.1,axes=ax)\nax.set_aspect(1)\n\n# Plot test points\ngroups = test.groupby('Speed')\n# The next step is to cycle through the groups (based on our categories) and plot each one on the same axis.\nfor name, group in groups:\n    ax.plot(group['Grade'], group['Bumpiness'], marker='o', linestyle='', ms=4, label=name)\nax.legend(bbox_to_anchor=(1.0,0.5))\nax.set_xlabel('Grade')\nax.set_ylabel('Bumpiness')\n\nacc_score = metrics.accuracy_score(labels_test, y_pred)\nmatt_score = metrics.matthews_corrcoef(labels_test, y_pred)\nprint(\"Accuracy Score: {}\".format(acc_score))\nprint(\"Matthews Correlation Coefficient (MCC): {}\".format(matt_score))"}
{"cell_type":"markdown","metadata":{},"source":"As we saw before, the perceptron is simple, fast, and not very accurate. It gives us a straight line and a binary output: is the value fast or slow? There are many times when we want more information than this: we would like to know with what probability the model predicts the object to be fast or slow? What if we could slightly blur the line between the \"slow\" category and the \"fast\" category so that we make a smooth transition from a 0% chance of being fast to a 100% chance of being fast. Our next model does exactly that.\n\n### Logistic Regression\n\nThis model will create a \"soft\" boundary between the different classes, giving us a smooth transition. This will be very important as we move towards neural networks: we need a smooth probability function to make them work. We'll use the same dataset and look at the logistic regression predictions."}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy Score: 0.935\nMatthews Correlation Coefficient (MCC): 0.8620192068216507\n"},{"data":{"image/png":"smc-blob::6f227253-7b83-4fc4-b6c4-4f42092f731a","text/plain":"<matplotlib.figure.Figure at 0x7fbb7b7362e8>"},"metadata":{},"output_type":"display_data"}],"source":"# Load the model and fit the data\nLRmodel = LogisticRegression(C=1e5)\nLRmodel.fit(features_train,labels_train)\n\ny_pred = LRmodel.predict(features_test)\n\n# Predict the boundary\nZ = pd.Series(LRmodel.predict(np.c_[xx.ravel(), yy.ravel()]), dtype='category').cat.codes.values.reshape(xx.shape)\n\n# First plot our points\ntestfig1, ax = plt.subplots()\n\nplt.pcolormesh(xx, yy, Z, cmap= plt.cm.cool, alpha=0.1,axes=ax)\nax.set_aspect(1)\n\n# Plot test points\ngroups = test.groupby('Speed')\n# The next step is to cycle through the groups (based on our categories) and plot each one on the same axis.\nfor name, group in groups:\n    ax.plot(group['Grade'], group['Bumpiness'], marker='o', linestyle='', ms=4, label=name)\nax.legend(bbox_to_anchor=(1.0,0.5))\nax.set_xlabel('Grade')\nax.set_ylabel('Bumpiness')\n\nacc_score = metrics.accuracy_score(labels_test, y_pred)\nmatt_score = metrics.matthews_corrcoef(labels_test, y_pred)\nprint(\"Accuracy Score: {}\".format(acc_score))\nprint(\"Matthews Correlation Coefficient (MCC): {}\".format(matt_score))"}
{"cell_type":"markdown","metadata":{},"source":"We see that the model only did slightly better at predicting the output classes compared to the perceptron. But the real gain is in the probability distribution. We can look at that specifically to see how it is now a smooth transition."}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7fbb2fe16080>"},"execution_count":4,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::829df5cb-6864-499a-82bb-8955271a1266","text/plain":"<matplotlib.figure.Figure at 0x7fbb74122400>"},"metadata":{},"output_type":"display_data"}],"source":"Z = pd.Series(LRmodel.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,0]).values.reshape(xx.shape)\nplt.pcolormesh(xx, yy, Z, cmap= plt.cm.YlGn)\ncbr = plt.colorbar()\ncbr.set_label('Probability of \"fast\"')\nplt.xlabel('Grade')\nplt.ylabel('Bumpiess')\n"}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"We see that there are large regions where the probabilities are pretty flat, but right near the decision boundary there is a smooth transition up from $0$ to $1$. This is just what we want.\n\n### Image Class Prediction\n\nEven though our model isn't very sophisticated, we can still use it to train a model to predict the class for a set of images. We'll use the MNIST data that we first saw way back in Class01. Here's a quick review of what the data look like."}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Expected Digit: 3\n"},{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fbb30285898>"},"execution_count":5,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"<matplotlib.figure.Figure at 0x7fbb302c64e0>"},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"smc-blob::40d8a7af-43b7-4b58-9858-cd4b2c42a681","text/plain":"<matplotlib.figure.Figure at 0x7fbb302a3c88>"},"metadata":{},"output_type":"display_data"}],"source":"digitDF = pd.read_csv('../Class01/Class01_digits_data.csv')\ntestnum = 61\n\ntestimage = digitDF.loc[testnum][0:64].values.reshape((8,8))\nprint('Expected Digit: {0:d}'.format(digitDF['target'][testnum]))\nplt.gray() \nplt.matshow(testimage) "}
{"cell_type":"markdown","metadata":{},"source":"We'll now split the data in to train/test sections and train a logistic regression."}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy Score: 0.9166666666666666\n"},{"data":{"image/png":"smc-blob::d56ab1fc-7ac3-4fdd-a747-e2e3a1aaa205","text/plain":"<matplotlib.figure.Figure at 0x7fbb30293e80>"},"metadata":{},"output_type":"display_data"}],"source":"digittrain, digittest = train_test_split(digitDF, test_size=0.2, random_state=23)\nfeatures = digittrain.ix[:, digittrain.columns != 'target'].values\nlabels = digittrain['target'].values\nclass_names = range(10) # class labels are just 0-9\n\nlogreg = LogisticRegression(C=1e5)\nlogreg.fit(features,labels)\n\ntestinputs = digittest.ix[:, digittest.columns != 'target'].values\npredictions = logreg.predict(testinputs)\nactuals = digittest['target'].values\n\ncnf_matrix = metrics.confusion_matrix(actuals, predictions)\n\ndef show_confusion_matrix(cnf_matrix, class_labels):\n    plt.matshow(cnf_matrix,cmap=plt.cm.YlGn,alpha=0.7)\n    ax = plt.gca()\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks(range(0,len(class_labels)))\n    ax.set_xticklabels(class_labels)\n    ax.set_ylabel('Actual Label', fontsize=16, rotation=90)\n    ax.set_yticks(range(0,len(class_labels)))\n    ax.set_yticklabels(class_labels)\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n\n    for row in range(len(cnf_matrix)):\n        for col in range(len(cnf_matrix[row])):\n            ax.text(col, row, cnf_matrix[row][col], va='center', ha='center', fontsize=16)\n        \nshow_confusion_matrix(cnf_matrix,class_labels=class_names)\nscore = metrics.accuracy_score(actuals, predictions)\nprint(\"Accuracy Score: {}\".format(score))"}
{"cell_type":"markdown","metadata":{},"source":"So we have a model with an accuracy score of 0.92 and a confusion matrix that looks pretty good. Because we have access to the prediction probabilities, we can look at the logloss metric, too."}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Log loss: 0.8077865731388394\n"}],"source":"predict_prob = logreg.predict_proba(testinputs)\nlogloss = metrics.log_loss(actuals, predict_prob)\nprint(\"Log loss: {}\".format(logloss))"}
{"cell_type":"markdown","metadata":{},"source":"The logloss is decent, given that this is a simple model.\n\n## Assignment\n\nThe assignment this week is to work on your project. I would like a draft version of your project turned in next week. We will spend some time in class next week doing a peer review of your work. This is a first draft, so I don't expect everything to be in its final form, but I do want you to fill in the basic outline of your project."}