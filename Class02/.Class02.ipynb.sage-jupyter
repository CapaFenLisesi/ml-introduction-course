{"kernelspec":{"display_name":"Python 3 (Anaconda)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 02\n## Machine Learning Models: Linear regression & Validation\n\nWe are going to cover two main topics in this class: **Linear Regressions** and **Validation**. We need to start with a broader question, though.\n\n## What is Machine Learning?\n\nThe goal this semester is to use machine learning to teach the computer how to make predictions. So we'll start with my definitions of machine learning -- in particular of supervised machine learning. We are using a programming algorithm that gives the computer the tools it needs to identify patterns in a set of data. Once we have those patterns, we can use them to make predictions - what we would expect should happen if we gather more data that may not necessarily be exactly the same as the data we learned from.\n\nWe'll start by looking at a very simple set of fake data that will help us cover the key ideas. Suppose that we just collected four data points. I've manually input them (as opposed to using a CSV file). Execute the following cell to see what the data look like."}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f1448b5d748>"},"execution_count":1,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::91c86fe0-f785-41a2-a994-138a130a558b","text/plain":"<matplotlib.figure.Figure at 0x7f14644f9710>"},"metadata":{},"output_type":"display_data"}],"source":"import pandas as pd\n\nfakedata1 = pd.DataFrame( \n       [[ 0.862,  2.264],\n       [ 0.694,  1.847],\n       [ 0.184,  0.705],\n       [ 0.41 ,  1.246]], columns=['input','output'])\n\nfakedata1.plot(x='input',y='output',kind='scatter')"}
{"cell_type":"markdown","metadata":{},"source":"It is pretty clear that there is a linear trend here. If I wanted to predict what would happen if we tried the input of `x=0.6`, it would be a good guess to pick something like `y=1.6` or so. Training the computer to do this is what we mean by _Machine Learning_. \n\nTo formalize this a little bit, it consists of four steps:\n\n1. We start with relevant historical data. This is our input to the machine learning algorithm.\n2. Choose an algorithm. There are a number of possibilities that we will cover over the course of the semester.\n3. Train the model. This is where the computer learns the pattern.\n4. Test the model. We now have to check to see how well the model works.\n\nWe then refine the model and repeat the process until we are happy with the results.\n\n### The Testing Problem\n\nThere is a bit of a sticky point here. If we use our data to train the computer, what do we use to test the model to see how good it is? If we use the same data to test the model we will, most likely, get fantastic results! After all, we used that data to train the model, so it should (if the model worked at all) do a great job of predicting the results.\n\nHowever, this doesn't tell us anything about how well the model will work with a _**new**_ data point. Even if we get a new data point, we won't necessarily know what it is _supposed_ to be, so we won't know how well the model is working. There is a way around all of this that works reasonably well. What we will do is set aside a part of our historical data as \"test\" data. We won't use that data to train the model. Instead, we will use it to test the model to see how well it works. This gives us a good idea of how the model will work with new data points. As a rule of thumb, we want to reserve about 20% of our data set as testing data.\n\nThere is a library that does this for us in Python called `train_test_split`. The documentation is here: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html. I want you to get used to looking up the documentation yourself to see how the function works. Pay close attention to the inputs and the outputs of the function. \n\nOne of the inputs we will use is the `random_state` option. By using the same number here we should all end up with the same results. If you change this number, you change the random distribution of the data and, thus, the end result."}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f1444a54668>"},"execution_count":2,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::41a65e59-3a3c-41d4-bc26-9febf0c19588","text/plain":"<matplotlib.figure.Figure at 0x7f1444053dd8>"},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"smc-blob::daddc954-edc9-431b-b062-facdbcae00b6","text/plain":"<matplotlib.figure.Figure at 0x7f1444053e80>"},"metadata":{},"output_type":"display_data"}],"source":"from sklearn.model_selection import train_test_split\n\nfaketrain1, faketest1 = train_test_split(fakedata1, test_size=0.2, random_state=23)\nfaketrain1.plot(x='input',y='output',kind='scatter')\nfaketest1.plot(x='input',y='output',kind='scatter')"}
{"cell_type":"markdown","metadata":{},"source":"You can see that, with a 20% split, our small fake dataset doesn't have very many points. Really we shouldn't be working with less than 100 points for anything we do. Any fewer than that and the statistics just start breaking. Ideally we'd have tens of millions of data points. We'll talk later about how to get that much data, but we'll start small for now. We'll load in the `Class02_fakedata2.csv` file and split it 80/20 training/testing datasets."}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f14354e3780>"},"execution_count":3,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::d6ab734a-1733-4319-bca4-b44863b15bf1","text/plain":"<matplotlib.figure.Figure at 0x7f1444cc6240>"},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"smc-blob::5437af5f-9925-4f9b-af1b-4207ce13c9f3","text/plain":"<matplotlib.figure.Figure at 0x7f1464a42b38>"},"metadata":{},"output_type":"display_data"}],"source":"fakedata2 = pd.read_csv('Class02_fakedata2.csv')\nfaketrain2, faketest2 = train_test_split(fakedata2, test_size=0.2, random_state=23)\nfaketrain2.plot(x='input',y='output',kind='scatter')\nfaketest2.plot(x='input',y='output',kind='scatter')"}
{"cell_type":"markdown","metadata":{},"source":"# Linear Regression\n\nWe are now ready to train our linear model on the training part of this data. Remember that, from this point forward, we must \"lock\" the testing data and not use it to train our models. This takes two steps in Python. The first step is to define the model and set any model parameters (in this case we'll use the defaults). This is a Python object that will subsequently hold all the information about the model including fit parameters and other information about the fit. Again, take a look at the documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html.\n\nThe second step is to actually fit the data. We need to reformat our data so that we can tell the computer what our inputs are and what our outputs are. We define two new variables called \"features\" and \"labels\". Note the use of the double square bracket in selecting data for the features. This will allow us to, in the future, select mutltiple columns as our input variables. In the mean time, it formats the data in the way that the fit algorithm needs it to be formatted."}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::0a59922f-5ab2-4247-b6e7-3768b879bfa7","text/plain":"    input  output\n85  0.833   2.243\n28  0.752   2.038\n8   0.609   1.645\n11  0.922   2.398\n63  0.067   0.467"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"faketrain2.head()"}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"from sklearn.linear_model import LinearRegression\n\n# Step 1: Create linear regression object\nregr = LinearRegression()\n\n# Step 2: Train the model using the training sets\nfeatures = faketrain2[['input']].values\nlabels = faketrain2['output'].values\n\nregr.fit(features,labels)"}
{"cell_type":"markdown","metadata":{},"source":"We now want to see what this looks like!  We start by looking at the fit coefficient and intercept. When we have more than one input variable, there will be a coefficient corresponding to each feature."}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Coefficients: \n [ 2.2882464]\nIntercept: \n 0.291018422471\n"}],"source":"print('Coefficients: \\n', regr.coef_)\nprint('Intercept: \\n', regr.intercept_)"}
{"cell_type":"markdown","metadata":{},"source":"That doesn't really tell us much. It would be better if we could compare the model to the test data. We will use the inputs from the test data and run them through the model. It will predict what the outputs should be. We can then compare them to the actual outputs. We'll plot the predictions as a line (since they will all lie on the same line due to our model being a linear regression)."}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7f142f23e518>"},"execution_count":7,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::6956f86f-9541-4447-a51e-aba528946009","text/plain":"<matplotlib.figure.Figure at 0x7f142f2ab6a0>"},"metadata":{},"output_type":"display_data"}],"source":"testinputs = faketest2[['input']].values\npredictions = regr.predict(testinputs)\nactuals = faketest2['output'].values\n\nimport matplotlib.pyplot as plt\nplt.scatter(testinputs, actuals, color='black', label='Actual')\nplt.plot(testinputs, predictions, color='blue', linewidth=1, label='Prediction')\n\n# We also add a legend to our plot. Note that we've added the 'label' option above. This will put those labels together in a single legend.\nplt.legend(loc='upper left', shadow=False, scatterpoints=1)\nplt.xlabel('input')\nplt.ylabel('output')"}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.legend.Legend at 0x7f142f28de48>"},"execution_count":8,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::171d262e-291d-4608-8f86-404515e189bc","text/plain":"<matplotlib.figure.Figure at 0x7f142f28de80>"},"metadata":{},"output_type":"display_data"}],"source":"plt.scatter(testinputs, (actuals-predictions), color='green', label='Residuals just because $\\lambda$')\nplt.xlabel('input')\nplt.ylabel('residuals')\nplt.legend(loc='upper left', shadow=False, scatterpoints=1)"}
{"cell_type":"markdown","metadata":{},"source":"This looks pretty good. We can go one step futher and define a quantitative measure of the quality of the fit. We will subtract the difference between the prediction and the actual value for each point. We then square all of those and average them.  Finally we take the square root of all of that. This is known as the RMS error (for Root Mean Squared)."}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"RMS Error: 0.041\n"}],"source":"import numpy as np\nprint(\"RMS Error: {0:.3f}\".format( np.sqrt(np.mean((predictions - actuals) ** 2))))"}
{"cell_type":"markdown","metadata":{},"source":"# Using Multiple Inputs\n\nWe'll now move to a real-world data set (which means it is messy). We'll load in the diabetes data set from Class 01 and try training it. Our input will be the 'BMI' feature and the output is the 'Target' column.\n"}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::536628b9-f232-4ec3-9c65-cc2c900dd7cc","text/plain":"smc-blob::8921bd4b-4cf5-49a8-a39c-5f9dd21b7054"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"diabetes = pd.read_csv('../Class01/Class01_diabetes_data.csv')\ndiabetes.head()"}
{"cell_type":"markdown","metadata":{},"source":"I've put all the steps together in one cell and commented on each step."}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"RMS Error: 62.352\n"},{"data":{"image/png":"smc-blob::b75a56ec-ae77-4358-a021-2fe33c8d5541","text/plain":"<matplotlib.figure.Figure at 0x7f142f193c88>"},"metadata":{},"output_type":"display_data"}],"source":"# Step 1: Split off the test data\ndia_train, dia_test = train_test_split(diabetes, test_size=0.2, random_state=23)\n\n# Step 2: Create linear regression object\ndia_model = LinearRegression()\n\n# Step 3: Train the model using the training sets\nfeatures = dia_train[['BMI']].values\nlabels = dia_train['Target'].values\n\n# Step 4: Fit the model\ndia_model.fit(features,labels)\n\n# Step 5: Get the predictions\ntestinputs = dia_test[['BMI']].values\npredictions = dia_model.predict(testinputs)\nactuals = dia_test['Target'].values\n\n# Step 6: Plot the results\nplt.scatter(testinputs, actuals, color='black', label='Actual')\nplt.plot(testinputs, predictions, color='blue', linewidth=1, label='Prediction')\nplt.xlabel('BMI') # Label the x axis\nplt.ylabel('Target') # Label the y axis\nplt.legend(loc='upper left', shadow=False, scatterpoints=1)\n\n# Step 7: Get the RMS value\nprint(\"RMS Error: {0:.3f}\".format( np.sqrt(np.mean((predictions - actuals) ** 2))))"}
{"cell_type":"markdown","metadata":{},"source":"Not too surprising that the RMS error isn't very good. This is the real world after all. However, we saw in Class 01 that there may be some dependence on some of the other variables like the LDL. We can try a linear regression with both of them as inputs. I have to change the code a little to do this. Compare this with the previous cell to see what needs to change."}
{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"RMS Error: 60.602\n"},{"data":{"image/png":"smc-blob::463b3fbc-4f28-45d8-860e-930afe27f9a0","text/plain":"<matplotlib.figure.Figure at 0x7f1444a5a898>"},"metadata":{},"output_type":"display_data"}],"source":"# Step 2: Create linear regression object\ndia_model2 = LinearRegression()\n\n# Possible columns:\n# 'Age', 'Sex', 'BMI', 'BP', 'TC', 'LDL', 'HDL', 'TCH', 'LTG', 'GLU'\n#\ninputcolumns = [ 'BMI', 'HDL']\n\n# Step 3: Train the model using the training sets\nfeatures = dia_train[inputcolumns].values\nlabels = dia_train['Target'].values\n\n# Step 4: Fit the model\ndia_model2.fit(features,labels)\n\n# Step 5: Get the predictions\ntestinputs = dia_test[inputcolumns].values\npredictions = dia_model2.predict(testinputs)\nactuals = dia_test['Target'].values\n\n# Step 6: Plot the results\n#\n# Note the change here in how we plot the test inputs. We can only plot one variable, so we choose the first.\n# Also, it no longer makes sense to plot the fit points as lines. They have more than one input, so we only visualize them as points.\n#\n\nplt.scatter(testinputs[:,0], actuals, color='black', label='Actual')\nplt.scatter(testinputs[:,0], predictions, color='blue', label='Prediction')\nplt.legend(loc='upper left', shadow=False, scatterpoints=1)\n\n# Step 7: Get the RMS value\nprint(\"RMS Error: {0:.3f}\".format( np.sqrt(np.mean((predictions - actuals) ** 2))))"}
{"cell_type":"markdown","metadata":{},"source":"## In-class Activity\n\nTry adding columns to the inputcolumns variable and see how that changes the RMS error. Some of the columns reduce the error significantly. If we use all of them, we reduce our RMS error by about 15%."}
{"cell_type":"markdown","metadata":{},"source":"# Assignment\n\nYour assignment is to try a linear regression on your own data set. Try adjusting the number of features you use to see if it improves your fit."}
{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":""}