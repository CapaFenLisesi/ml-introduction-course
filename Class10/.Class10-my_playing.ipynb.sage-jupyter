{"kernelspec":{"display_name":"Anaconda (Python 3)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 10\n## ML Techniques: Feature scaling\n\nAnother aspect of optimizing machine learning algorithms is to think about feature scaling. When we use multiple numeric features as inputs to a regression or classification algorithm, the computer just sees those values as numbers without context or units. What if we have the data that has one column corresponding to a driver's age and another column that corresponds to the vehicle gross weight. Those are very different sets of numbers. Without doing any other scaling or pre-processing, the machine learning algorithm may emphasize the vehicle weights more than the ages because they are larger numbers. We don't want that to happen.\n\nThe process of changing the data scale is called feature scaling and we'll look at two different types of feature scaling. If you want a good in-depth tutorial on how this works, I [recommend this tutorial](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html).\n\n## Working with Different Data Types\n\nWe'll use a sample dataset from the [UCI archives.](http://archive.ics.uci.edu/ml/datasets/Wine) This dataset is looking at various characteristics of different wine samples and has classified the samples into one of three different classes (1,2, and 3). Let's import the data and take a look at it.\n"}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::bf6978ed-acb5-4881-9b04-b123673c2ec3","text/plain":"smc-blob::fcd57d78-909c-42f9-88f2-311c742fc3bc"},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":"import pandas as pd\n\ndf = pd.read_csv('Class10_wine_data.csv')\n\ndf.head()"}
{"cell_type":"markdown","metadata":{},"source":"There are a number of different features here. We'll focus on the first two: Alcohol which has units (percent/volumne) and Malic acid with units (g/l). Any machine learning algorithm that uses these features is going to treat them as if they were on the same scale. However, they aren't. So we need to re-scale them to unitless values in order to work with them.\n\nLet's first look at the distribution of points in those columns."}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::dadb1cb3-4f3a-4615-bbfb-206a9544c321","text/plain":"<matplotlib.figure.Figure at 0x7f2c6443c7b8>"},"metadata":{},"output_type":"display_data"}],"source":"# Plot the first two feature columns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\n\n#plt.figure(figsize=(8,6))\n\nplt.scatter(df['Alcohol'], df['Malic acid'])\nplt.xlabel('Alcohol (%/L)')\nplt.ylabel('Malic Acid (g/L)')\nplt.xlim(0,16)\nplt.ylim(0,16)\nplt.axes().set_aspect('equal')"}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<seaborn.axisgrid.PairGrid at 0x7f2c3f2787f0>"},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::177a2ee1-ecb6-44a2-a9d7-99756feec853","text/plain":"<matplotlib.figure.Figure at 0x7f2c3f2789b0>"},"metadata":{},"output_type":"display_data"}],"source":"sns.pairplot(df, hue=\"Wine Class\")"}
{"cell_type":"code","execution_count":19,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size=0.2, random_state=23)\n\nminmax_scaler = MinMaxScaler().fit(train[['Alcohol' ,'Malic acid']])\ntrain_features = minmax_scaler.transform(train[['Alcohol' ,'Malic acid']])\n\ntest_features = minmax_scaler.transform(test[['Alcohol' ,'Malic acid']])\n\ntrain_target = train['Wine Class']\ntest_target = test['Wine Class']"}
{"cell_type":"code","execution_count":31,"metadata":{"collapsed":false,"trusted":true},"outputs":[],"source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size=0.2, random_state=23)\n\nminmax_scaler = MinMaxScaler().fit(train.drop('Wine Class',axis=1))\ntrain_features = minmax_scaler.transform(train.drop('Wine Class',axis=1))\n\ntest_features = minmax_scaler.transform(test.drop('Wine Class',axis=1))\n\ntrain_target = train['Wine Class']\ntest_target = test['Wine Class']"}
{"cell_type":"code","execution_count":32,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"from sklearn.naive_bayes import GaussianNB\nnb_model = GaussianNB()\nnb_model.fit(train_features, train_target)\ny_pred_nb = nb_model.predict(test_features)"}
{"cell_type":"code","execution_count":33,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"[[11  0  0]\n [ 0 15  0]\n [ 0  0 10]]\n"}],"source":"from sklearn.metrics import confusion_matrix\n\ncnf_matrix = confusion_matrix(test_target, y_pred_nb)\nprint(cnf_matrix)"}
{"cell_type":"code","execution_count":34,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Accuracy Score: 1.0\n"}],"source":"import sklearn.metrics as metrics\nacc_score = metrics.accuracy_score(test_target, y_pred_nb)\nprint(\"Accuracy Score: {}\".format(acc_score))"}
{"cell_type":"markdown","metadata":{},"source":"### Standarization scaling\n\nAs we noted above, the goal is to turn these features into unitless parameters. That means we have to divide the feature by a quantity with the same units. There are two typical ways of doing this: we'll start with **standardization** or **Z-scale normalization**. We need to calculate the standard deviation of the feature, then divide the entire column by the standard deviation. Because the standard deviation has the same units as the feature itself, this takes care of the normalization.\n\nWe'll also do one more thing: we'll subtract the mean value of the distribution from each data point first. That way we end up with a distribution that is centered at zero. Of course there [is a library](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) that will do this for us. We need to fit the library on the data we want to scale, then use the `transform()` function to transform the input data based on the scaler. We can do both features at the same time: the scaler keeps track of them individually."}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Mean before standardization:\nAlcohol=13.00, Malic acid=2.34\n\nMean after standardization:\nAlcohol=-0.00, Malic acid=-0.00\n\nStandard deviation before standardization:\nAlcohol=0.81, Malic acid=1.12\n\nStandard deviation after standardization:\nAlcohol=1.00, Malic acid=1.00\n"}],"source":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler().fit(df[['Alcohol', 'Malic acid']])\ndf_std = std_scaler.transform(df[['Alcohol', 'Malic acid']])\n\nprint('Mean before standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n      .format(df['Alcohol'].mean(), df['Malic acid'].mean()))\n\nprint('\\nMean after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n      .format(df_std[:,0].mean(), df_std[:,1].mean()))\n\nprint('\\nStandard deviation before standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n      .format(df['Alcohol'].std(), df['Malic acid'].std()))\n\nprint('\\nStandard deviation after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n      .format(df_std[:,0].std(), df_std[:,1].std()))\n"}
{"cell_type":"markdown","metadata":{},"source":"We've accomplished what we set out to do: the new means are close to zero and the standard deviations are 1. Let's see how it changed the shape of the distributions."}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::538f15c8-3885-4917-96a7-533ff752a8e6","text/plain":"<matplotlib.figure.Figure at 0x7f2c3f47dd30>"},"metadata":{},"output_type":"display_data"}],"source":"fig, ax = plt.subplots(1,2)\n\nfor a,d,l in zip(range(len(ax)),\n               (df[['Alcohol', 'Malic acid']].values, df_std),\n               ('Input scale',\n                'Standardized [$N  (\\mu=0, \\; \\sigma=1)$]')\n                ):\n    for i,c in zip(range(1,4), ('red', 'blue', 'green')):\n        ax[a].scatter(d[df['Wine Class'].values == i, 0],\n                  d[df['Wine Class'].values == i, 1],\n                  alpha=0.5,\n                  color=c,\n                  label='Class %s' %i\n                  )\n        ax[a].set_aspect('equal')\n    ax[a].set_title(l)\n    ax[a].set_xlabel('Alcohol')\n    ax[a].set_ylabel('Malic Acid')\n    ax[a].legend(loc='upper left')\n    ax[a].grid()\nplt.tight_layout()"}
{"cell_type":"markdown","metadata":{},"source":"It looks like we've re-scaled the data without changing the basic shape or the relationships between the points. That's good. The standardized data can now be used as inputs for the machine learning algorithms.\n\n### Min-Max Scaling\n\nOne of the things we didn't mention before was that the standardization scaling only really works if we have enough data to get a good standard deviation and if the data are normally distributed (i.e. they look like a bell-shaped curve). IF these things don't apply, then standardization may not be the best thing to do.\n\nThere is another way we could scale the data: we could figure out the \"distance\" between the maximum and the minimum and then scale the data based on this \"distance\". Of course we should also subtract off the minimum point. The net result is that now the entire dataset lies between 0 and 1. Let's do this and see how it looks."}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Mean before min-max scaling:\nAlcohol=13.00, Malic acid=2.34\n\nMean after min-max scaling:\nAlcohol=0.52, Malic acid=0.32\n\nStandard deviation before min-max scaling:\nAlcohol=0.81, Malic acid=1.12\n\nStandard deviation after min-max scaling:\nAlcohol=0.21, Malic acid=0.22\n"}],"source":"from sklearn.preprocessing import MinMaxScaler\n\nminmax_scaler = MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])\ndf_minmax = minmax_scaler.transform(df[['Alcohol', 'Malic acid']])\n\nprint('Mean before min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n      .format(df['Alcohol'].mean(), df['Malic acid'].mean()))\n\nprint('\\nMean after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n      .format(df_minmax[:,0].mean(), df_minmax[:,1].mean()))\n\nprint('\\nStandard deviation before min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n      .format(df['Alcohol'].std(), df['Malic acid'].std()))\n\nprint('\\nStandard deviation after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n      .format(df_minmax[:,0].std(), df_minmax[:,1].std()))"}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"array([[ 0.34473684,  0.33794466],\n       [ 0.16578947,  0.22529644]])"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":"train_features[0:2]"}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Alcohol</th>\n      <th>Malic acid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>162</th>\n      <td>12.85</td>\n      <td>3.27</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>13.73</td>\n      <td>1.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"     Alcohol  Malic acid\n162    12.85        3.27\n30     13.73        1.50"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":"test[['Alcohol' ,'Malic acid']].head(2)"}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"array([[ 0.47894737,  0.5       ],\n       [ 0.71052632,  0.15019763]])"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"test_features = minmax_scaler.transform(test[['Alcohol', 'Malic acid']])\ntest_features[0:2]"}
{"cell_type":"markdown","metadata":{},"source":"As you can see, we've really shifted things around. The means are not very pretty (because the dataset has been shrunk to fit between 0 and 1). Additionally the standard deviation has changed and isn't very pretty, either. Let's plot it to see if the shape has changed."}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::ec940adb-c1eb-42af-a8fb-f07f920a6714","text/plain":"<matplotlib.figure.Figure at 0x7f2c3f280240>"},"metadata":{},"output_type":"display_data"}],"source":"fig, ax = plt.subplots(1,2)\n\nfor a,d,l in zip(range(len(ax)),\n               (df[['Alcohol', 'Malic acid']].values, df_minmax),\n               ('Input scale',\n                'Min-max scale')\n                ):\n    for i,c in zip(range(1,4), ('red', 'blue', 'green')):\n        ax[a].scatter(d[df['Wine Class'].values == i, 0],\n                  d[df['Wine Class'].values == i, 1],\n                  alpha=0.5,\n                  color=c,\n                  label='Class %s' %i\n                  )\n        ax[a].set_aspect('equal')\n    ax[a].set_title(l)\n    ax[a].set_xlabel('Alcohol')\n    ax[a].set_ylabel('Malic Acid')\n    ax[a].legend(loc='upper left')\n    ax[a].grid()\nplt.tight_layout()"}
{"cell_type":"markdown","metadata":{},"source":"Although it looks like we've squished the dataset, the basic relationships between the points look the same - the overall shape is pretty close. \n\nWhether you use the standardization or the min-max scaling will depend on your dataset and your machine learning algorithm. I recommend exploring both to see which performs better for your data.\n\n## In-class Activity\n\nThere are more features in the wine dataset. Run through the exercise of normalizing a couple of more features and plot them to make sure that you did it right. Finally, try running a classifier algorithm on the data both with and without normalization. How does the outcome change (if at all)?"}
{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"-"}
{"cell_type":"markdown","metadata":{},"source":"## Homework\n\nTest feature normalization on your project data. Does it change the outcome of your predictions? Document your results."}