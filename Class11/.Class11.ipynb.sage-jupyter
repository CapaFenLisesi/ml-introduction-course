{"kernelspec":{"display_name":"Python 3 (Anaconda)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 11\n## ML Techniques: PCA\n\nToday we look at a technique that can be useful in reducing the number or dimensionality of input features. The classic example of this is in doing machine learning with images. Consider an image that is 200 pixels by 200 pixels. We would like to use information from each pixel as the inputs for a machine learning algorithm. That means we have 40,000 input features! That is more than most learning algorithms can handle.\n\nHowever, we notice that a fair number of the pixels in our images are black or similarly colored. We also notice that there are some similar features in all of the images. What if we could shrink or compress the number of input features to describe the images? That's what Principal Component Analysis (PCA) does.\n\nWe'll start by looking at the iris data that we saw way back in Class 01. We will look at what the PCA is doing and how it works using this data. Then we'll move to working with some image data.\n\n### PCA with the Iris Dataset\n\nI'm following [this tutorial](https://plot.ly/ipython-notebooks/principal-component-analysis/) for this section. Refer to the tutorial for more information on PCA and how it works. I also suggest looking at [this tutorial](http://napitupulu-jon.appspot.com/posts/pca-ud120.html) which uses a more qualitative explanation on how PCA works.\n\nWe'll start by loading the dataset and taking a look at it. I remmber that we renamed the columns to make it easier to work with the data, so I copied that piece from Class01, too."}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::5fc13192-659c-40ff-ac90-e23210d2b893","text/plain":"   sepalLen  sepalWid  petalLen  petalWid       target\n0       5.0       2.3       3.3       1.0  Versicolour\n1       5.7       2.9       4.2       1.3  Versicolour\n2       4.7       3.2       1.6       0.2       Setosa\n3       7.7       3.0       6.1       2.3    Virginica\n4       5.5       2.5       4.0       1.3  Versicolour"},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nirisdf = pd.read_csv('../Class01/Class01_iris_data.csv')\nirisdf.columns=['sepalLen','sepalWid','petalLen','petalWid','target']\nirisdf.head()"}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<seaborn.axisgrid.PairGrid at 0x7f86f2711cf8>"},"execution_count":2,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::6d282a5c-0ee5-43d6-8384-0ebfd7b6aa68","text/plain":"<matplotlib.figure.Figure at 0x7f86f2711d68>"},"metadata":{},"output_type":"display_data"}],"source":"# Plot the data to see what we are working with\nsns.pairplot(irisdf, hue='target')"}
{"cell_type":"markdown","metadata":{},"source":"What we see here is that we really have four different input variables and that the output classifications have different distributions in all four variables. Take a look at the diagonal plots in the `pairplot` graph- those are the histograms of each output class for the variable (either column or row- they are the same on the diagonal). We would like to take this four-dimensional data and see if we can rework it down to two dimensions. For example, it looks like petal length and petal width are related to each other. Maybe we can collapse that data into a single dimension. Perhap a similar thing could be done with sepal length and sepal width. Fortunately, we don't have to manually decide how to collapse the data - the algorithm will find the best combination and use it.\n\nWe'll use the [Scikit Learn PCA too](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Be sure to take a look at the documentation. We'll do a train/test split here and work with just the training data. As always, we do that as early in the process as we can.\n\nThe one input paramter for the `PCA()` tool that we care about right now is the final number of components. This is asking the algorithm to reduce our dataset down to just $n$ components. We'll pick $n=2$ to start with and see what that does to the data. The `fit()` function calculates how to transform the data. The `transform` function then gives us a new set of features that are transformed by the algorithm."}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"from sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA \nimport numpy as np\n# Split up the features and the labels columns\nfeatures = irisdf[['sepalLen','sepalWid','petalLen','petalWid']]\nlabels = irisdf['target']\n\n# Do our train/test split on the data\ntrain_features, test_features, train_labels, test_labels= train_test_split(features, labels, test_size=0.2, random_state=23)\n\n# Try using the PCA: first we train the transformation using the fit function, then we transform the data.\nsklearn_pca = PCA(n_components=2)\nsklearn_pca.fit(train_features)\npca_train_features = sklearn_pca.transform(train_features)"}
{"cell_type":"markdown","metadata":{},"source":"Now that we've transformed the training features, let's take a look at what we got. Let's start by looking at the output from the `transform()` function."}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"array([[ 3.46540921,  0.53900563],\n       [-2.47634639,  0.53756189],\n       [ 1.23856377, -0.18361792],\n       [ 2.39392551, -0.2620317 ]])"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"pca_train_features[1:5]"}
{"cell_type":"markdown","metadata":{},"source":"It looks like it is an array of numbers that appear to by our $(x,y)$ coordinates. Let's try plotting this to see what we've got."}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7f86d0185710>"},"execution_count":5,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::c863efbd-cfb9-4939-86bf-9a1eb9bc7d2c","text/plain":"<matplotlib.figure.Figure at 0x7f86d91366d8>"},"metadata":{},"output_type":"display_data"}],"source":"# We can turn the labels into a categorical column, then get the category codes to use them to plot the different labels.\nylabels = train_labels.astype('category').cat.codes\n\n# Make the scatter plot and label the axis\nplt.scatter(pca_train_features[:,0],pca_train_features[:,1],c=ylabels,cmap=plt.cm.spectral)\nplt.xlabel('PCA_0')\nplt.ylabel('PCA_1')"}
{"cell_type":"markdown","metadata":{},"source":"We now have just two input features to work with: `PCA_0` and `PCA_1`. Any future machine learning will now be simplified especially since we see that the data are mostly separated in this two-dimensional space.\n\nWe'll run through a comparison to see how the PCA has changed the performance of a couple of different machine learning algorithms that we've seen before (Suppor Vector Classifiers and Decision Tree Classifiers). Remember that the PCA features are half the size of the original dataset (we've shrunk them down to 2 columns instead of 4). Before we fit the data, we need to transform the test features using the PCA tranformation we trained earlier. Remember that we didn't use the test data to train the PCA, so we haven't cheated on this one."}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"pca_test_features = sklearn_pca.transform(test_features)"}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"SVC on complete dataset score: 0.9333333333333333\nSVC on PCA dataset score     : 0.9333333333333333\nDTC on complete dataset score: 0.9333333333333333\nDTC on PCA dataset score     : 0.9333333333333333\n"}],"source":"# Import both the SVC and DTC classifiers\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Train a basic SVC (default values) on the entire dataset and score it\nsvcmodel = SVC()\nsvcmodel.fit(train_features,train_labels)\nprint(\"SVC on complete dataset score: {}\".format(svcmodel.score(test_features, test_labels)))\n\n# Train a similar SVC model using the PCA reduced features and score it\nsvcmodelPCA = SVC()\nsvcmodelPCA.fit(pca_train_features,train_labels)\nprint(\"SVC on PCA dataset score     : {}\".format(svcmodelPCA.score(pca_test_features, test_labels)))\n\n# Try the decision tree version on the entire model\ndtmodel = DecisionTreeClassifier(random_state=32)\ndtmodel.fit(train_features,train_labels)\nprint(\"DTC on complete dataset score: {}\".format(dtmodel.score(test_features,test_labels)))\n\ndtmodelPCA = DecisionTreeClassifier(random_state=32)\ndtmodelPCA.fit(pca_train_features,train_labels)\nprint(\"DTC on PCA dataset score     : {}\".format(dtmodelPCA.score(pca_test_features,test_labels)))\n"}
{"cell_type":"markdown","metadata":{},"source":"Wow! The PCA dataset scores the same as the full datasets using both machine learning algorithms. Now we haven't optimized anything, but this is a good indication that the PCA is doing its job: shrinking the number of input features while keeping the performance of the machine learning algorithm.\n\n## Working with Facial Images\n\nNow that we've got a basic idea on how the PCA works and when to use it, let's try using it on image data. We are interested in training a machine learning algorithm to recognize the faces of specific people: we provide a picture of a person and the machine tells us who it is. We're going to follow [this tutorial](http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html#sphx-glr-auto-examples-applications-face-recognition-py). I've already downloaded the data and stored it in a binary storage container (called a *pickle* in python). We'll load the data and take a look at what we've got.\n\n"}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"1140 images that are 50 pixels by 37 pixels in size with a total of 1850 features per image\n"}],"source":"import pickle\npeople = pickle.load(open('people.pkl', 'rb')) # This is how we load pickled data\n\n#Find out how many faces we have, and\n#the size of each picture from.\nn_images, h, w = people.images.shape\nprint(\"{0} images that are {1} pixels by {2} pixels in size with a total of {3} features per image\".format(n_images,h,w,h*w))"}
{"cell_type":"markdown","metadata":{},"source":"So our machine learning algorithm would have to train on 1850 different features! Let's take a look at a couple of the images. We'll first get out the images and their labels. How to do this comes from the [documentation for this dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html)."}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"['Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder'\n 'Tony Blair']\n"}],"source":"# First we get the list of features\nimagefeatures = people.data\n# And the list of labels - this is a set of numbers (0-4) corresponding to the 5 different people\nimagelabels = people.target\n# We'd like to get the people's names, too.\ntarget_names = people.target_names\nprint(target_names)\n"}
{"cell_type":"markdown","metadata":{},"source":"Let's image this and see what the images look like. We will pick a random person, list their name, and show their picture. We can rerun the cell to get different random pictures."}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Person 695 :George W Bush\n"},{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f86d004f518>"},"execution_count":10,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::4125d43a-1c86-4667-a3be-51b8b496a552","text/plain":"<matplotlib.figure.Figure at 0x7f86d004f4e0>"},"metadata":{},"output_type":"display_data"}],"source":"pickaperson = np.random.randint(0,n_images)\nsns.set_style('white')\npersonname = target_names[imagelabels[pickaperson]]\nprint(\"Person {} :\".format(pickaperson) + personname)\npersonimage = imagefeatures[pickaperson].reshape(h,w)\nplt.axis('off')\nplt.imshow(personimage,cmap=plt.cm.gray)\n"}
{"cell_type":"markdown","metadata":{},"source":"Ok, back to business. Let's split off our training/testing dataset. We want to be a little careful about this: we don't want to accidentally get all images of one person in our training dataset. We can split it based on the label so that we have equal distributions of all the different classes. We use the `stratify=imagelabels` option that tells `train_test_split` to use equal numbers of the different classes in each split.\n\nWe will then find the principal components using the PCA analysis. In this case we'll use 150 components. So the net effect is that we are shrinking the number of features from 1850 to 150.\n\nWe're using two more options for the PCA: when working with images, I've found that the performance is better when we use the `randomized` SVD solver and set the `whiten=True` option  ([see this page for more information](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))."}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"trusted":true},"outputs":[],"source":"# split into a training and testing set\nfeatures_train, features_test, labels_train, labels_test = train_test_split(imagefeatures, imagelabels, stratify=imagelabels, test_size=0.20, random_state=23)\n \n# Compute the PCA (eigenfaces) on the face dataset\nn_components = 150\n \nimgpca = PCA(n_components=n_components,svd_solver='randomized',\n          whiten=True).fit(features_train)\n\npca_features_train = imgpca.transform(features_train)"}
{"cell_type":"markdown","metadata":{},"source":"We can look at the way in which the PCA has broken up the images- in this case it is something like looking at which axis (or eigenvector if you've had linear algebra) it used. These components tell us something about the features that were most common to all of the images. That is why the PCA algorithm chose them."}
{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::06ad6c07-d7a6-4496-85e9-76c00aa7a025","text/plain":"<matplotlib.figure.Figure at 0x7f86caaf80b8>"},"metadata":{},"output_type":"display_data"}],"source":"# First get the components and reshape them to the right size for images\neigenfaces = imgpca.components_.reshape((n_components, h, w))\n\n# A helper function to make plots of the components\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=5):\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99,\n                        top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n \n# Plot the gallery of the most significative eigenfaces\neigenface_titles = [\n    \"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n \nplot_gallery(eigenfaces, eigenface_titles, h, w)\nplt.tight_layout()"}
{"cell_type":"markdown","metadata":{},"source":"We'll come back to these images in a little bit- they give us the start of an algorithm to detect faces out of other images.\n\nNow that we've got the PCA features, let's try an run a machine learning algorithm to see if we can detect the faces. We'll try using both the complete dataset as well as the PCA reduced data set. This time let's compare how long it takes to train the model.\n"}
{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Done in 4.763s with accuracy 0.465\n"}],"source":"from time import time\nt0 = time()\nsvcmodel = SVC().fit(features_train, labels_train)\naccuracy_score = svcmodel.score(features_test,labels_test)\nprint(\"Done in {0:.3f}s with accuracy {1:.3f}\".format(time() - t0,accuracy_score))"}
{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"done in 0.551s with accuracy 0.838\n"}],"source":"pca_features_test = imgpca.transform(features_test)\n\nt0 = time()\nsvcmodel_pca = SVC().fit(pca_features_train, labels_train)\naccuracy_score_pca = svcmodel_pca.score(pca_features_test,labels_test)\nprint(\"done in {0:.3f}s with accuracy {1:.3f}\".format(time() - t0,accuracy_score_pca))"}
{"cell_type":"markdown","metadata":{},"source":"Here is where we see the power of the PCA: not only did it take 1/10th the time to train the model, but we got a better result! That speedup means we can potentially deal with significantly larger sets of input features without having to wait forever for the output.\n\n### An Aside: using GridSearchCV to optimize Hyperparameters.\n\nOk, we did pretty well using the default hyperparameters for the SVC. But, as we know, we can often do better by tuning them. One way to do this is to create a list of possibilities and let the computer try all combinations and tell us which one was best. Because we've sped up the training, we can do this. We're going to use the `GridSearchCV` function. We first need to define the parameters we'll be scanning and what values we want to scan over."}
{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"done in 21.075s\nBest estimator found by grid search:\nSVC(C=1000.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma=0.005, kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\nNew accuracy score: 0.8991228070175439\n"}],"source":"from sklearn.model_selection import GridSearchCV\n\nt0 = time()\n# Define my parameters and the values we want to try\nparam_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n\n# We then call the GridSearchCV, tell it what machine learning algorithm to use, and what parameters to scan\nclf = GridSearchCV(SVC(), param_grid)\n\n# Like other functions, we now fit it.\nclf = clf.fit(pca_features_train, labels_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best estimator found by grid search:\")\nprint(clf.best_estimator_)\n\naccuracy_score_pca = clf.score(pca_features_test,labels_test)\nprint(\"New accuracy score: {}\".format(accuracy_score_pca))"}
{"cell_type":"markdown","metadata":{},"source":"So we are now running with about 89% accuracy in identifying the person in the picture. Not bad for a couple of seconds of training! I'm curious as to the confusion matrix: are we equally good at identifying all five of the people? Who are we confusing?"}
{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::b535ee51-c5d3-403b-842a-647893e5cc5f","text/plain":"<matplotlib.figure.Figure at 0x7f86d011f630>"},"metadata":{},"output_type":"display_data"}],"source":"from sklearn.metrics import confusion_matrix\n\nlabels_pred = clf.predict(pca_features_test)\ncnf_matrix = confusion_matrix(labels_test, labels_pred)\n\ndef show_confusion_matrix(cnf_matrix, class_labels):\n    plt.matshow(cnf_matrix,cmap=plt.cm.YlGn,alpha=0.7)\n    ax = plt.gca()\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks(range(0,len(class_labels)))\n    ax.set_xticklabels(class_labels,rotation=45)\n    ax.set_ylabel('Actual Label', fontsize=16, rotation=90)\n    ax.set_yticks(range(0,len(class_labels)))\n    ax.set_yticklabels(class_labels)\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n\n    for row in range(len(cnf_matrix)):\n        for col in range(len(cnf_matrix[row])):\n            ax.text(col, row, cnf_matrix[row][col], va='center', ha='center', fontsize=16)\n        \nshow_confusion_matrix(cnf_matrix,target_names)"}
{"cell_type":"markdown","metadata":{},"source":"As we saw with the accuracy score, we're doing pretty good. There doesn't appear to be a lot of obvious mistakes here: It looks like Tony Blair was mistaken for some of the other people, but nothing stands out dramatically.\n\n## Computer Vision: Finding Faces in an Image\n\nAlthough this is not a class on computer vision, there is a connection between the two. We may want to use images as our input data. If we can reduce the size of the features by, for example, identifying the face out of an image, we can reduce the work we have to do by focusing on the face. The way we identify faces is connected to the eigenface plot we made above. The basic trick is to search for features that are common to all human faces. Then, once we found some of the basic features, we iteratively look more closely at those areas of the image, honing in on the face.\n\nThere are some good resouces on how this works [here](http://www.cs.unc.edu/~lazebnik/spring09/lec22_eigenfaces.pdf), [here](http://www.cs.unc.edu/~lazebnik/spring09/lec23_face_detection.pdf), and [here](http://www.cs.unc.edu/~lazebnik/spring09/).\n\nWe can test out the image recognition using the OpenCV (cv2) package for python.\n\n### Haar Cascade\n\nThe tool we want to use is called the Haar Cascade classifier. We'll need to load it in, then use it to find faces in a picture.\n"}
{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"(-0.5, 279.5, 238.5, -0.5)"},"execution_count":17,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::20f08acb-4ff7-4e7d-9545-9af6525b626b","text/plain":"<matplotlib.figure.Figure at 0x7f86caac04a8>"},"metadata":{},"output_type":"display_data"}],"source":"import cv2\nimport matplotlib.pyplot as plt\n# Load the classifier\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n# read the image\nimg = cv2.imread('ABBA.png')\n\n# make sure it is grayscale - the classifier does not work on color images\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\nplt.imshow(gray,cmap=plt.cm.gray)\nplt.axis('off')"}
{"cell_type":"markdown","metadata":{},"source":"Now we want to detect the faces in the image. We'll apply the facial detection algorithm.\n"}
{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Found 4 faces\n"}],"source":"# Detect faces in the image\nfaces = face_cascade.detectMultiScale(\n    gray,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30))\nprint(\"Found {0} faces\".format(len(faces)))"}
{"cell_type":"markdown","metadata":{},"source":"Now that we've found faces in the image, we can identify where they are at and put rectangles around them. We'll use the OpenCV package for adding a rectangle around each of the detected face regions."}
{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"(-0.5, 279.5, 238.5, -0.5)"},"execution_count":19,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::dfec125b-1cd0-4399-a7a2-f06f95260ece","text/plain":"<matplotlib.figure.Figure at 0x7f86ca592438>"},"metadata":{},"output_type":"display_data"}],"source":"roi_gray=[]\n\nfor (x,y,w,h) in faces:\n    roi_gray.append(gray[y:y+h, x:x+w])\n    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n\nplt.imshow(img)\nplt.axis('off')"}
{"cell_type":"code","execution_count":20,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"(-0.5, 44.5, 44.5, -0.5)"},"execution_count":20,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::17aecc63-fbf7-4f8f-9601-5483d175ce62","text/plain":"<matplotlib.figure.Figure at 0x7f86ca68f320>"},"metadata":{},"output_type":"display_data"}],"source":"# We can also look at the individual faces:\nfacenumber = 0\nplt.imshow(roi_gray[facenumber],cmap=plt.cm.gray)\nplt.axis('off')"}
{"cell_type":"markdown","metadata":{},"source":"## In-class Activities\n\nI've got two things for you to work on: first, let's join the facial detection algorithm with the facial recognition algorithm. Find a picture of one of the five people we used in the facial detection (preferably a picture of more than just their face so it will be interesting).\n\nRun the facial detection algorithm and find their face out of the photo. We'll need to crop and re-shape the image in order to use it with our facial detection algorithm. In the end we need an image that is 37 pixels wide and 50 pixels tall. Once we have that, test to see if the trained model can recognize our new face. \n\n## Assignment\n\nI want you to try out the PCA analysis on your own dataset. Reduce the dimensionality of your features and see how that changes the performance score and the time required to fit your model.\n"}