{"kernelspec":{"display_name":"Python 3 (Anaconda)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 03\n## Big Data Cleaning: Data Transformations\n\nAlthough machine learning is the exciting part of this course, most data scientists spend the vast majority of their time doing data clearning and data wrangling. Some put the figure at as high as 90% of their time! There is a good reason for this: most of the data out there is not in a format needed for the machine learning algorithms. So, in order to do machine learning, the data must be reorganized, cleaned, rearranged, normalized, enriched, and filtered. We'll begin this process today and continue working on it through the course."}
{"cell_type":"markdown","metadata":{},"source":"### Feature Types\n\nWe start with an overview of some of the types of features we could potentially use. In the end, all of the data are represented as bits in the computer (ones and zeros), but we can organize those bits in a bunch of different ways in the pandas dataframes. We'll build a \"fake\" dataframe with the different types in them.\n\n#### Integers\n\nIntegers are counting numbers and other whole numbers (including negatives): ...,-4,-3,-2,-1,0,1,2,3,4,... They are somewhat special because they can be stored very efficiently and the computer can operate on them very efficiently (positive integers especially). Pandas stores these using a data type called **int64** where the 64 means they are 64-bit integers (capable of storing any number between -9,223,372,036,854,775,807 and 9,223,372,036,854,775,807)\n\nWe'll use a sample dataset to look at the different types of data as we go."}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"IntCol        int64\nFloatCol    float64\nTextCol      object\nCatCol       object\nDateCol      object\nLatCol      float64\nLonCol      float64\ndtype: object\n\nInteger Values\n[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"}],"source":"import pandas as pd\n\nsampledata = pd.read_csv('Class03_sample_dataframe.csv')\n\n# This will let us look at the data type of each column. Note that the first column is an \"int64\".\nprint(sampledata.dtypes)\n\n# These are the values stored in this column.\nprint(\"\\nInteger Values\")\nprint(sampledata['IntCol'].values)"}
{"cell_type":"markdown","metadata":{},"source":"#### Floating point numbers\n\nFloating point numbers, or decimal numbers are just that: any number with a decimal place in it such as 4.566642 and -156.986714. Pandas stores these as a **float64**. They could also be stored in scientific notation like this: 4.509013e+14. This means \"4.509013 times 10 raised to the +14\". These are still floating point numbers and are treated like any other decimal number."}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Float Values\n[ 1.34846527  1.65852321  1.99091463  2.15807893  2.50018684  2.60270486\n  2.79540154  3.01384446  3.1916722   3.25200862  3.46085057  3.6586824\n  3.79061854  3.83351212  3.94635944  4.15475126  4.27976091  4.27608798\n  4.44673513  4.59670764]\n"}],"source":"print(\"Float Values\")\nprint(sampledata['FloatCol'].values)"}
{"cell_type":"markdown","metadata":{},"source":"Before we move on, I'd like to take a quick look at the data graphically."}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f20a0058860>"},"execution_count":3,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::a90c8607-8b4c-4b82-8098-8d24585aeb5d","text/plain":"<matplotlib.figure.Figure at 0x7f20a4f03940>"},"metadata":{},"output_type":"display_data"}],"source":"sampledata.plot(kind='scatter', x='IntCol',y='FloatCol')"}
{"cell_type":"markdown","metadata":{},"source":"Because this is \"fake\" data, I put in a functional dependence here. The float column looks like it is some function of the integer column. It is almost always a good idea to visualize your data early on to see what it looks like graphically!"}
{"cell_type":"markdown","metadata":{},"source":"#### Text\n\nPandas can store text in its columns. Because there are a number of different types of text objects, by default pandas will store text as an **object** which just means it doesn't know which of the types it really is. Text can, in principle, be anything you want it to be, so it is both the most flexible and the most challenging data type."}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Text Values\n['cat' 'dog' 'horse' 'cow' 'elephant' 'fish' 'bird' 'dinosaur' 'giraffe'\n 'wolf' 'prairie dog' 'whale' 'dolphin' 'clam' 'lizard' 'snake' 'fly'\n 'beetle' 'spider' 'worm']\n"}],"source":"print(\"Text Values\")\nprint(sampledata['TextCol'].values)"}
{"cell_type":"markdown","metadata":{},"source":"#### Categorical\n\nA categorical data type is a finite set of different objects. These objects are represented internally as integers but may be displayed as text or other generic objects. To make things simple, we'll start with a categorical object that has three possible values: \"yes\", \"no\", and \"maybe\". Internally, pandas will represent these as integers 0,1, and 2. But it knows that this is a categorical data type, so it keeps track of the text value associated with the integer and displays that for the user."}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Categorical Values\n['no' 'no' 'yes' 'no' 'yes' 'no' 'yes' 'yes' 'maybe' 'no' 'no' 'no' 'no'\n 'no' 'yes' 'maybe' 'no' 'yes' 'yes' 'yes']\n"}],"source":"print(\"Categorical Values\")\nprint(sampledata['CatCol'].values)"}
{"cell_type":"markdown","metadata":{},"source":"When we loaded the data, it actually loaded this column as an **object**, which means it doesn't know that it is supposed to be a categorical column. We will tell pandas to do that. We will use the `astype()` command that will tell pandas to change the data type of that column. We check to make sure it worked, too. Note that the \"CatCol2\" column is now a 'category' type.\n\n_**Data Processing Tip**_\n\nA quick aside here: there are a couple of ways of doing this kind of transformation on the data. We'll see this a little later when we do more column-wise processing. We could either change the original column or we could create a new column. The second method doesn't overwrite the original data and will be what we typically do. That way if something goes wrong or we want to change how we are processing the data, we still have the original data column to work with."}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"IntCol         int64\nFloatCol     float64\nTextCol       object\nCatCol        object\nDateCol       object\nLatCol       float64\nLonCol       float64\nCatCol2     category\ndtype: object"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"sampledata[\"CatCol2\"] = sampledata[\"CatCol\"].astype('category')\nsampledata.dtypes"}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"IntCol           int64\nFloatCol       float64\nTextCol         object\nCatCol          object\nDateCol         object\nLatCol         float64\nLonCol         float64\nCatCol2       category\nFloatToInt       int64\ndtype: object"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":"sampledata[\"FloatToInt\"] = sampledata[\"FloatCol\"].astype('int')\nsampledata.dtypes"}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::ab14f6db-3e1b-4076-932f-26f3afef3054","text/plain":"   FloatCol  FloatToInt\n0  1.348465           1\n1  1.658523           1\n2  1.990915           1\n3  2.158079           2\n4  2.500187           2"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":"sampledata[['FloatCol','FloatToInt']].head()"}
{"cell_type":"markdown","metadata":{},"source":"We can now look at how the data are stored as categorical data. We can get thi internal codes for each of the entries like this:"}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"0     1\n1     1\n2     2\n3     1\n4     2\n5     1\n6     2\n7     2\n8     0\n9     1\n10    1\n11    1\n12    1\n13    1\n14    2\n15    0\n16    1\n17    2\n18    2\n19    2\ndtype: int8"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"sampledata[\"CatCol2\"].cat.codes"}
{"cell_type":"markdown","metadata":{},"source":"We can also get a list of the categories that pandas found when converting the column. These are in order- the first entry corresponds to 0, the second to 1, etc."}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"Index(['maybe', 'no', 'yes'], dtype='object')"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"sampledata[\"CatCol2\"].cat.categories"}
{"cell_type":"markdown","metadata":{},"source":"We may encounter situations where we want to plot the data and visualize each category as its own color. We saw how to do this back in Class01."}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<seaborn.axisgrid.FacetGrid at 0x7f20c0447b00>"},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::86e1d4f4-74ac-4599-9bbf-d626516d5394","text/plain":"<matplotlib.figure.Figure at 0x7f208c43bb38>"},"metadata":{},"output_type":"display_data"}],"source":"import seaborn as sns\nsns.set_style('white')\nsns.lmplot(x='IntCol', y='FloatCol', data=sampledata, hue='CatCol2', fit_reg=False)"}
{"cell_type":"markdown","metadata":{},"source":"#### Date/Times\n\nWe will frequently encounter date/time values in working with data. There are many different ways that these values get stored, but mostly we'll find that they start as a text object. We need to know how they are stored (in what order are the year-month-day-hour-minute-second values are stored). There are utilities to convert any type of date/time string to a datetime object in pandas. We will start with the ISO 8601 datetime standard, since it is both the most logical and the easiest to work with. Dates are stored like this: **2017-01-23** where we use a four-digit year, then a two-digit month and a two-digit day, all separated by dashes. If we want to add a time, it is appended to the date like this: **2017-01-23T03:13:42**. The \"T\" tells the computer that we've added a time. Then it is followed by a two-digit hour (using 00 as midnight and 23 as 11pm) a colon, a two-digit minute, a colon, and a two-digit second. There are other variations of this that can include a time-zone, but we will leave those for later. "}
{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Date/Time Values\n['2017-01-25T11:10:15' '2017-01-26T00:06:43' '2017-01-26T21:11:33'\n '2017-01-27T06:44:09' '2017-01-27T20:55:49' '2017-01-28T03:26:11'\n '2017-01-28T17:50:22' '2017-01-29T04:02:27' '2017-01-29T15:19:37'\n '2017-01-29T18:03:21' '2017-01-30T00:06:48' '2017-01-30T14:10:13'\n '2017-01-31T10:56:47' '2017-01-31T12:03:30' '2017-01-31T18:25:47'\n '2017-02-01T16:11:57' '2017-02-02T10:41:38' '2017-02-02T23:53:35'\n '2017-02-03T22:16:36' '2017-02-04T06:41:42']\n"}],"source":"print(\"Date/Time Values\")\nprint(sampledata['DateCol'].values)"}
{"cell_type":"markdown","metadata":{},"source":"They are currently stored as **objects**, not as datetimes. We need to convert this column as well, but we'll use a special pandas function to do that. Take a quick look at the [reference page](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html) for this function to see what else it can do. Note that the new column has type **datetime64[ns]**. That means that the date format is capable of counting nanoseconds. We won't use all of that capability, but pandas used that format because our dates are accurate to the second."}
{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"IntCol                 int64\nFloatCol             float64\nTextCol               object\nCatCol                object\nDateCol               object\nLatCol               float64\nLonCol               float64\nCatCol2             category\nFloatToInt             int64\nDateCol2      datetime64[ns]\ndtype: object"},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":"sampledata[\"DateCol2\"] = pd.to_datetime(sampledata[\"DateCol\"])\nsampledata.dtypes"}
{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"smc-blob::9a759fc6-bf8d-452f-aaa5-3cc28b56713c"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":"#We print out the column to see what it looks like\nsampledata[\"DateCol2\"]"}
{"cell_type":"markdown","metadata":{},"source":"Now that we have the datetime column, I'd like to plot the data as a function of date. This is often a useful thing to do with time series data. We'll need to import the matplotlib library and use a trick to format the data by date. Here's the code that makes it work."}
{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::eed9a886-bc11-46f8-9610-5e962a788fad","text/plain":"<matplotlib.figure.Figure at 0x7f209b50c748>"},"metadata":{},"output_type":"display_data"}],"source":"import matplotlib.pyplot as plt\n# We will plot the data values and set the linestyle to 'None' which will not plot the line. We also want to show the individual data points, so we set the marker.\nplt.plot(sampledata['DateCol2'].values, sampledata['FloatCol'].values, linestyle='None', marker='o')\n# autofmt_xdate() tells the computer that it should treat the x-values as dates and format them appropriately. This is a figure function, so we use gcf() to \"get current figure\"\nplt.gcf().autofmt_xdate()"}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"#### Geographical\n\nAlthough this is not typically a single data type, you may encounter geographical data. These are typically in a Latitude-Longitude format where both Latitude and Longitude are floating point numbers like this: (32.1545, -138.5532). There are a number of tools we can use to work with and plot this type of data, so I wanted to cover it now. For now, we will treat these as separate entities and work with geographical data as we encounter it."}
{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Latitude Values\n[-39.49065628 -70.12948911 -16.13173975  87.89217686  70.44943608\n  58.05434711  -2.53972797 -62.7995723  -82.55162128 -70.17320869\n -25.36587139 -75.60602277  -7.53093654 -42.9814863   66.79396011\n  25.04139344   1.83167685   8.00208991  33.87311843  37.14139842]\nLongitude Values\n[ 155.028043    -38.5430472    29.02027996  -88.75756107  -83.95066221\n -115.3764318   -68.05589951  -29.09891648  -45.48397025   79.36955726\n -154.2330359     8.00061215 -173.3395272   -75.00614985 -113.6252933\n   -1.75222417  -33.07273958  138.9382069   102.8687652    69.72581269]\n"}],"source":"print(\"Latitude Values\")\nprint(sampledata['LatCol'].values)\nprint(\"Longitude Values\")\nprint(sampledata['LonCol'].values)"}
{"cell_type":"markdown","metadata":{},"source":"It is also useful to plot the geographical data. There are python libraries that make this easy to do."}
{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":"/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:1656: MatplotlibDeprecationWarning: The get_axis_bgcolor function was deprecated in version 2.0. Use get_facecolor instead.\n  fill_color = ax.get_axis_bgcolor()\n/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:1800: MatplotlibDeprecationWarning: The get_axis_bgcolor function was deprecated in version 2.0. Use get_facecolor instead.\n  axisbgc = ax.get_axis_bgcolor()\n/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:3327: MatplotlibDeprecationWarning: The ishold function was deprecated in version 2.0.\n  b = ax.ishold()\n/projects/anaconda3/lib/python3.5/site-packages/mpl_toolkits/basemap/__init__.py:3336: MatplotlibDeprecationWarning: axes.hold is deprecated.\n    See the API Changes document (http://matplotlib.org/api/api_changes.html)\n    for more details.\n  ax.hold(b)\n"},{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f208b0517f0>]"},"execution_count":17,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::dd12b6ed-94e5-4146-9731-24824042e977","text/plain":"<matplotlib.figure.Figure at 0x7f208d11f390>"},"metadata":{},"output_type":"display_data"}],"source":"from mpl_toolkits.basemap import Basemap\nimport numpy as np\n\n# Draw the base map of the world\nm = Basemap(projection='robin',lon_0=0,resolution='c')\n# Draw the continent coast lines\nm.drawcoastlines()\n# Color in the water and the land masses\nm.fillcontinents(color='red',lake_color='aqua')\n# draw parallels and meridians.\nm.drawparallels(np.arange(-90.,120.,30.))\nm.drawmeridians(np.arange(0.,360.,60.))\n#m.drawmapboundary(fill_color='aqua')\n\n# Prep the data for plotting on the map\nx,y = m(sampledata['LonCol'].values, sampledata['LatCol'].values)\n# Plot the data points on the map\nm.plot(x,y, 'bo', markersize=10)"}
{"cell_type":"markdown","metadata":{},"source":"### Column-wise processing\n\nNow that we have data columns, we've already seen a couple of examples of column-wise processing. When we created the categorical column and the datetime column we took the data from one column and operated on it all at the same time creating the new columns with the different data types. There are other ways to manipulate the columns.\n\n#### apply\n\nThe `apply` function takes each entry in a column and *applies* whatever function you want to the entry. For example, we are interested in whether the entry is greater than 4. We will simplify the code by using what is called a **`lambda`** function. So, inside the `apply()` function we have: `lambda x: x>4`. This is shorthand notation for the following:\n\n\"Treat `x` as if it were each entry in the column. Apply whatever follows the colon (:) to each entry and create a new column based on the output\". The use of `x` was arbitrary: we could choose any variable. For example if we chose `w`, the code would read: `lambda w: w>4`. This would do exactly the same thing."}
{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"    FloatCol GTfour\n0   1.348465  False\n1   1.658523  False\n2   1.990915  False\n3   2.158079  False\n4   2.500187  False\n5   2.602705  False\n6   2.795402  False\n7   3.013844  False\n8   3.191672  False\n9   3.252009  False\n10  3.460851  False\n11  3.658682  False\n12  3.790619  False\n13  3.833512  False\n14  3.946359  False\n15  4.154751   True\n16  4.279761   True\n17  4.276088   True\n18  4.446735   True\n19  4.596708   True\n"}],"source":"sampledata['GTfour'] = sampledata['FloatCol'].apply(lambda x: x > 4.0)\nprint(sampledata[['FloatCol','GTfour']])"}
{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"    FloatCol   FloatCC\n0   1.348465  1.348465\n1   1.658523  1.658523\n2   1.990915  1.990915\n3   2.158079  0.000000\n4   2.500187  0.000000\n5   2.602705  0.000000\n6   2.795402  0.000000\n7   3.013844  0.000000\n8   3.191672  0.000000\n9   3.252009  0.000000\n10  3.460851  0.000000\n11  3.658682  0.000000\n12  3.790619  0.000000\n13  3.833512  0.000000\n14  3.946359  0.000000\n15  4.154751  0.000000\n16  4.279761  0.000000\n17  4.276088  0.000000\n18  4.446735  0.000000\n19  4.596708  0.000000\n"}],"source":"sampledata['FloatCC'] = sampledata['FloatCol'].apply(lambda cellentry: cellentry if cellentry < 2 else 0)\nprint(sampledata[['FloatCol','FloatCC']])"}
{"cell_type":"markdown","metadata":{},"source":"#### Common functions\n\nThere are a number of common functions that we could use inside the `apply`. For example, if we wanted to get the square root of each entry, this is what it would look like. We are using the function `np.sqrt` from the `numpy` library. We already imported this library, but if we didn't, we'd need to `import numpy as np` before running this function."}
{"cell_type":"code","execution_count":20,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"    FloatCol  FloatSQRT\n0   1.348465   1.161234\n1   1.658523   1.287837\n2   1.990915   1.410998\n3   2.158079   1.469040\n4   2.500187   1.581198\n5   2.602705   1.613290\n6   2.795402   1.671945\n7   3.013844   1.736043\n8   3.191672   1.786525\n9   3.252009   1.803333\n10  3.460851   1.860336\n11  3.658682   1.912768\n12  3.790619   1.946951\n13  3.833512   1.957936\n14  3.946359   1.986545\n15  4.154751   2.038321\n16  4.279761   2.068758\n17  4.276088   2.067870\n18  4.446735   2.108728\n19  4.596708   2.143993\n"}],"source":"sampledata['FloatSQRT'] = sampledata['FloatCol'].apply(np.sqrt)\nprint(sampledata[['FloatCol','FloatSQRT']])"}
{"cell_type":"markdown","metadata":{},"source":"Another useful function is adding up columns. Note that we need to tell pandas to run through each row by adding the argument `axis=1` to the `apply` function. Otherwise it tries to add up each column. This might be something you might want to do, too, though the easiest way to do that is to use the pandas `sum` function for the column."}
{"cell_type":"code","execution_count":21,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"    IntCol  FloatCol  IntSUM\n0        1  1.348465     NaN\n1        2  1.658523     NaN\n2        3  1.990915     NaN\n3        4  2.158079     NaN\n4        5  2.500187     NaN\n5        6  2.602705     NaN\n6        7  2.795402     NaN\n7        8  3.013844     NaN\n8        9  3.191672     NaN\n9       10  3.252009     NaN\n10      11  3.460851     NaN\n11      12  3.658682     NaN\n12      13  3.790619     NaN\n13      14  3.833512     NaN\n14      15  3.946359     NaN\n15      16  4.154751     NaN\n16      17  4.279761     NaN\n17      18  4.276088     NaN\n18      19  4.446735     NaN\n19      20  4.596708     NaN\n"}],"source":"sampledata['IntSUM'] = sampledata[['IntCol','FloatCol']].apply(np.sum,axis=0)\nprint(sampledata[['IntCol','FloatCol','IntSUM']])"}
{"cell_type":"code","execution_count":22,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"IntCol      210.000000\nFloatCol     64.955867\ndtype: float64"},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":"sampledata[['IntCol','FloatCol']].apply(np.sum,axis=0)"}
{"cell_type":"code","execution_count":23,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"210"},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":"sampledata['IntCol'].sum()"}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"#### Custom functions\n\nWe will now create our first custom function and use it to process the data. We will make a short function that will look to see if a value in the TextCol feature matches an item on a list we create."}
{"cell_type":"code","execution_count":24,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"        TextCol   IsMammal\n0           cat     mammal\n1           dog     mammal\n2         horse     mammal\n3           cow     mammal\n4      elephant     mammal\n5          fish  notmammal\n6          bird  notmammal\n7      dinosaur  notmammal\n8       giraffe     mammal\n9          wolf     mammal\n10  prairie dog     mammal\n11        whale     mammal\n12      dolphin     mammal\n13         clam  notmammal\n14       lizard  notmammal\n15        snake  notmammal\n16          fly  notmammal\n17       beetle  notmammal\n18       spider  notmammal\n19         worm  notmammal\n"}],"source":"\n# We first tell the computer that we are writing a function by starting with \"def\"\n# The next text is the name of the function. We name this one \"isMammal\" meaning it will tell us if an animal is in our list of mammals\n# The final text in the parenthesis is an input to the function. This is another \"dummy\" variable - we could give it any name we want. \n# In this case we call it \"animal\" to remind ourselves that we expect an animal type in text form.\ndef isMammal(animal):\n    # We create a list of text objects that will be our \"inclusive\" list. If the item is on this list, the function will return True. Otherwise it returns false.\n    mammallist = ['cat','dog','horse','cow','elephant','giraffe','wolf','prairie dog', 'whale', 'dolphin']\n    # This is our first \"if\" statement. What this particular version does is look at the list \"mammallist\". \n    # If the text passed into the variable \"animal\" matches any item in the list, it jumps into this next block of code\n    # Otherwise it jumps into block of code following the \"else\" statement\n    if animal in mammallist:\n        # the \"return\" code word tells the computer we are done and to send back to the apply function the value following \"return\". In this case, send back \"True\"\n        return 'mammal'\n    else:\n        # The other case will send back \"false\".\n        return 'notmammal'\n    \nsampledata['IsMammal'] = sampledata['TextCol'].apply(isMammal)\nprint(sampledata[['TextCol', 'IsMammal']])"}
{"cell_type":"code","execution_count":25,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"        TextCol  FloatCol IsSmallMammal\n0           cat  1.348465          True\n1           dog  1.658523          True\n2         horse  1.990915          True\n3           cow  2.158079         False\n4      elephant  2.500187         False\n5          fish  2.602705         False\n6          bird  2.795402         False\n7      dinosaur  3.013844         False\n8       giraffe  3.191672         False\n9          wolf  3.252009         False\n10  prairie dog  3.460851         False\n11        whale  3.658682         False\n12      dolphin  3.790619         False\n13         clam  3.833512         False\n14       lizard  3.946359         False\n15        snake  4.154751         False\n16          fly  4.279761         False\n17       beetle  4.276088         False\n18       spider  4.446735         False\n19         worm  4.596708         False\n"}],"source":"# We'll now operate on an entire row of data at once and do a more complicated operation. We'll return only mammals where the 'FloatCol' is smaller than 2.\n\ndef isMammalFloat(row):\n    # We create a list of text objects that will be our \"inclusive\" list. If the item is on this list, the function will return True. Otherwise it returns false.\n    mammallist = ['cat','dog','horse','cow','elephant','giraffe','wolf','prairie dog', 'whale', 'dolphin']\n    \n    # We need to identify the animal from the row - it can be addressed using the column name\n    animal = row['TextCol']\n    \n    if animal in mammallist:\n        # the \"return\" code word tells the computer we are done and to send back to the apply function the value following \"return\". \n        # In this case it returns True if the float value is less than 2 and false otherwise.\n        return row['FloatCol'] < 2\n    else:\n        # If it isn't a mammal, return false\n        return False\n\n# Note that we need to tell `apply` to send one row at a time by adding the `axis=1` argument\nsampledata['IsSmallMammal'] = sampledata.apply(isMammalFloat, axis=1)\nprint(sampledata[['TextCol', 'FloatCol','IsSmallMammal']])"}
{"cell_type":"code","execution_count":26,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"0      cat\n1      dog\n2    horse\nName: TextCol, dtype: object"},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":"sampledata['TextCol'][ sampledata['FloatCol']<2 ]"}
{"cell_type":"markdown","metadata":{},"source":"### Feature extraction\n\nWe can often pull additional features from what we currently have. This involves doing a column-wise processing step, but with the additional component of doing a transformation or extraction from the data. We'll look at a couple of techniques to do this.\n\n#### Date/day/week features\n\nWe already saw how to take a text column that is a date and turn it into a datetime data type. The `to_datetime()` function has the capability of parsing many different string formats. I recommend looking at the [documentation for the function](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html) to learn how to do parsing of more specific date time formats. \n\nOnce we have a datetime data type, we can use other functions to get, for example, the day of the week or the week of the year for any given date. This may be useful for looking at weekly patterns or yearly patterns. The full list of features we can easily extract is [found in the documentation](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#time-date-components). We use the `apply` function with the simple in-line `lambda` function to get the date or time features. Another use for this might be to identify holidays- for example, Memorial day is always on the same relative day of the year (last Monday in May). We could use these functions to identify which days are national or bank holidays."}
{"cell_type":"code","execution_count":27,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"    DayofWeek  WeekofYear\n0   Wednesday           4\n1    Thursday           4\n2    Thursday           4\n3      Friday           4\n4      Friday           4\n5    Saturday           4\n6    Saturday           4\n7      Sunday           4\n8      Sunday           4\n9      Sunday           4\n10     Monday           5\n11     Monday           5\n12    Tuesday           5\n13    Tuesday           5\n14    Tuesday           5\n15  Wednesday           5\n16   Thursday           5\n17   Thursday           5\n18     Friday           5\n19   Saturday           5\n"}],"source":"# Get the day of the week for each of the data features. We can get either a numerical value (0-6) or the names\nsampledata['DayofWeek'] = sampledata['DateCol2'].apply(lambda x: x.weekday_name)\n# Or the week number in the year\nsampledata['WeekofYear'] = sampledata['DateCol2'].apply(lambda x: x.week)\n\nprint(sampledata[['DayofWeek', 'WeekofYear']])"}
{"cell_type":"markdown","metadata":{},"source":"#### Unique values\n\nSometimes it is helpful to know what unique values are in a column. Especially when there are many rows (millions), it is impractical to manually scan through the columns to look for unique values. However, we can use a pandas function `unique()` to do just that. We will see this is particularly helpful in doing data cleaning to identify rows with problems in the data."}
{"cell_type":"code","execution_count":28,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"array(['no', 'yes', 'maybe'], dtype=object)"},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":"sampledata['CatCol'].unique()"}
{"cell_type":"markdown","metadata":{},"source":"#### Text regex features\n\nAnother type of text feature extraction using a `regex` or *regular expression* pattern recognition code. The date/time conversion uses one form of this, but we can be more general in identifying patterns. There are some very useful tools for testing your pattern. I like the tester at https://regex101.com/. I use it whenever I build a pattern recognition string. "}
{"cell_type":"code","execution_count":29,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::3bad23c6-ff6a-43b5-88b7-d079d283c1f8","text/plain":"         0\n0      NaN\n1      NaN\n2     hors\n3      NaN\n4       el\n5      NaN\n6      NaN\n7      NaN\n8   giraff\n9      NaN\n10  prairi\n11    whal\n12     NaN\n13     NaN\n14     NaN\n15    snak\n16     NaN\n17   beetl\n18    spid\n19     NaN"},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":"# This simple text pattern gathers all the letters up to (but not including) the last 'e' in the text entry. There are lots of other pattern recognition tools to extract features from text.\n# Note that it returns \"NaN\" if there are no 'e's in the text string. We could use that to find all the strings without an 'e' in them.\nsampledata['TextCol'].str.extract(\"(.*)e\", expand=True)"}
{"cell_type":"markdown","metadata":{},"source":"#### Converting to categorical\n\nWe already saw how to convert text columns to categorical columns. We can also covert other data types to categorical columns. For example, we could bin a float column into regularly sized bins, then create a categorical column from those bins.\n\n### Word/Text cleaning\n\nFinally, it is often useful to clean up text entries before trying to turn them into features. For example, we may want to remove all punctuation, capital letters, or other special characters. We may also want to consider all of the forms of a word as the same word. For example, we may want to have both \"dog\" and \"dogs\" as the same feature. Or we may want \"wonder\" and \"wonderful\" as the same feature. There are a couple of text processing tools in python that simplify this work considerably.\n\nI created a small dataset to work with. We'll use one of the rows to test our text cleaning process."}
{"cell_type":"code","execution_count":30,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"smc-blob::43c0e71c-2168-4f8b-812f-3c51f80c80fe"},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":"textDF = pd.read_csv('Class03_text.tsv',sep='\\t')\ntestcase = textDF['review'][3]\ntestcase"}
{"cell_type":"markdown","metadata":{},"source":"The first thing we notice is that there are hypertext bits in the text (the `<br />` items). We want to clean all of those out. The BeautifulSoup function does this for us."}
{"cell_type":"code","execution_count":31,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"smc-blob::a2aaddc0-10ed-4002-91a0-e55046fa49c4"},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":"from bs4 import BeautifulSoup\ncleantext = BeautifulSoup(testcase,\"html5lib\").text\ncleantext"}
{"cell_type":"markdown","metadata":{},"source":"We now want to get rid of everything that isn't an alphabetical letter. That will clean up all punctuation and get rid of all numbers. We'll use a regex substitution function to do this. It looks for everything that is not an alphabetical character and replaces it with a blank space."}
{"cell_type":"code","execution_count":32,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"smc-blob::877fa6e4-1769-41a1-9bfb-f9f7981a7c15"},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":"import re\nonlyletters = re.sub(\"[^a-zA-Z]\",\" \",cleantext)\nonlyletters"}
{"cell_type":"markdown","metadata":{},"source":"We'll get rid of upper-case letters to only look at the words themselves."}
{"cell_type":"code","execution_count":33,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"smc-blob::cbe56a95-1005-4eca-bf62-a3dc2ae0fcaf"},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":"lowercase = onlyletters.lower()\nlowercase"}
{"cell_type":"markdown","metadata":{},"source":"The next two steps we'll do at once because we need to split up the text into individual words to do them. The `split()` function breaks up the string into an array of words. We will then eliminate any words that are **stopwords** in English. These are words like \"and\", \"or\", \"the\" that don't communciate any information but are necessary for language.\n\nThe other thing we'll do is cut the words down to their root stems. This will get rid of plurals or other modifications of words."}
{"cell_type":"code","execution_count":34,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"went see film great deal excitement school director even good friend mine sorry mate film stinks talk wrong first half walked went pub much needed drink someone standing balcony jump send helicopter shine searchlight think nothing would make likely jump local radio send reporters cover people attempt suicide fear pressuring jumping fear encouraging copy cat instances whatever circumstances radio reporters live broadcasts th floor tower block radio cars carry leads long enough connect microphone headphones transmitter stuck lift scene utterly derivative acting direction almost non existent could go\n\n\nwent see film great deal excit school director even good friend mine sorri mate film stink talk wrong first half walk went pub much need drink someon stand balconi jump send helicopt shine searchlight think noth would make like jump local radio send report cover peopl attempt suicid fear pressur jump fear encourag copi cat instanc whatev circumst radio report live broadcast th floor tower block radio car carri lead long enough connect microphon headphon transmitt stuck lift scene utter deriv act direct almost non exist could go\n"}],"source":"import nltk\nfrom nltk.corpus import stopwords # Import the stop word list\n\nwords = lowercase.split() \nmeaningfulwords = [w for w in words if not w in stopwords.words(\"english\")]\n\nfrom nltk.stem import SnowballStemmer\nsnowball_stemmer = SnowballStemmer(\"english\")\n\nstemmedwords = [snowball_stemmer.stem(w) for w in meaningfulwords ]\n\nprint(\" \".join(meaningfulwords))\nprint(\"\\n\")\nprint(\" \".join(stemmedwords))"}
{"cell_type":"code","execution_count":35,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::b7fc0134-cc1d-47b8-a6a2-79f831699f7a","text/plain":"smc-blob::cff38e52-245f-4d14-ac03-91dfd516cff9"},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":"# Now we make a function that we can apply to every entry in the dataframe\n\ndef cleantext(textinput):\n    \n    # First Pass: remove any html tags\n    from bs4 import BeautifulSoup\n    cleantext = BeautifulSoup(textinput,\"html5lib\").text\n    \n    # Second pass: remove non-letters and make everything lower case\n    import re\n    testcase = re.sub(\"[^a-zA-Z]\",\" \",cleantext)\n    lowercase = testcase.lower()\n    \n    # Third pass: remove all stop words (non-essential words)\n    from nltk.corpus import stopwords # Import the stop word list\n    words = lowercase.split() \n    meaningfulwords = [w for w in words if not w in stopwords.words(\"english\")]\n\n    # Fourth pass: get the word stems so that plurals, etc. are reduced\n    from nltk.stem import SnowballStemmer\n    snowball_stemmer = SnowballStemmer(\"english\")\n    stemmedwords = [snowball_stemmer.stem(w) for w in meaningfulwords ]\n\n    # Put the words back together again with a single space beteen them\n    return \" \".join(stemmedwords)\n\ntextDF['cleaned'] = textDF['review'].apply(cleantext)\ntextDF"}
{"cell_type":"markdown","metadata":{},"source":"## Data Cleaning Example In-class Activity\n\nThe tutorial on cleaning messy data is located here: http://nbviewer.jupyter.org/github/jvns/pandas-cookbook/blob/v0.1/cookbook/Chapter%207%20-%20Cleaning%20up%20messy%20data.ipynb\n\nFollow the tutorial, looking at the data and how to do a preliminary clean to eliminate entries that aren't correct or don't help. The data file can be loaded from the SageMath folder. I've reduced the number of column features in the data set to make it a bit easier to work with."}
{"cell_type":"code","execution_count":36,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"requests = pd.read_csv(\"Class03_311_data.csv\")"}
{"cell_type":"markdown","metadata":{},"source":"# Assignment\n\nYour assignment is to do data processing and cleaning on your own dataset. I want documentation of what you've done and why you chose to do those things to your data. \n\nI would also like you to try redoing your regression from last week, using the new features that you create through the data processing steps. See if you can improve the quality of your regression.\n"}
{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":""}