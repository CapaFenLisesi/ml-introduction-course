{"kernelspec":{"display_name":"Python 3 (Anaconda)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.3"}}
{"cell_type":"markdown","metadata":{"collapsed":false},"source":"# Class 08\n## ML Techniques: Outliers\n\n"}
{"cell_type":"markdown","metadata":{},"source":"## Unsupervised Learning\n\nUp to this point we have been working with *supervised* learning - we have a set of features and labels (or outputs in the case of a regression) that we used to teach the machine. What if we don't have labels? What if all we have is a set of unlabeled data points? It is still possible to do some types of *unsupervised* machine learning. We'll cover a couple of these methods over the next few classes.\n\nWe'll begin with **Outlier Detection**. There are a couple of different ways we can think about outliers:\n\n1. An outlier is a _false_ data point that was erroneously recorded\n2. An outlier is a _true_ data point that indicates or records an event that is different than the others and thus of interest\n\nIf we are dealing with the first case: the outlier is not real, but was, for example, a mis-recorded or mis-entered number, than the outlier is something we need to **fix**. However, if it is the second case, the outlier may be the thing we are looking for. We'll work with both cases and talk about strategies for dealing with them. Let's first look at how to find the outliers in the data, then we'll look at what to do with them.\n\nI've prepared a sample dataset of fake data with a couple of outliers. Let's load the data and take a look at it graphically."}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff54017f320>"},"execution_count":1,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::ad34c877-9f5d-40cf-80eb-d882615a0bbc","text/plain":"<matplotlib.figure.Figure at 0x7ff52508cc50>"},"metadata":{},"output_type":"display_data"}],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndfa = pd.read_csv(\"Class08_fakedata1.csv\")\ndfa.plot.scatter(x='A',y='B')"}
{"cell_type":"markdown","metadata":{},"source":"It looks like there are a couple of data points that don't fit in with the rest of the group. Perhaps we can spot them by looking at just the 'A' column data?"}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff5258b83c8>"},"execution_count":2,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::8459a4f5-c18b-4903-b615-8c59675bac0f","text/plain":"<matplotlib.figure.Figure at 0x7ff526174780>"},"metadata":{},"output_type":"display_data"}],"source":"dfa['A'].plot()"}
{"cell_type":"markdown","metadata":{},"source":"It looks like one of the points is around the 100 row point. But where is the other? Let's try plotting the 'B' column."}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff520970c50>"},"execution_count":3,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::94a37764-df51-4a00-8da4-53f95c19df40","text/plain":"<matplotlib.figure.Figure at 0x7ff54023c128>"},"metadata":{},"output_type":"display_data"}],"source":"dfa['B'].plot()"}
{"cell_type":"markdown","metadata":{},"source":"Well, it isn't clear from this plot that there are any problems in the 'B' column. There is another way we can visualize the data: using a `boxplot`. This type of plot looks at the mean and standard deviation of the data. Then it provides a single plot that shows various limits based on these values. Here's an explanation of the ranges of data used by the boxplot ([explanation from here](http://stackoverflow.com/questions/17725927/boxplots-in-matplotlib-markers-and-outliers)). Any points outside of this range are plotted as separate data points and should be visible.\n\n![Boxplot explanation](Class08_boxplot_limits.png) "}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff52309eba8>"},"execution_count":4,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::08d55cfb-4057-4b58-a527-5b4531e77856","text/plain":"<matplotlib.figure.Figure at 0x7ff54054b160>"},"metadata":{},"output_type":"display_data"}],"source":"dfa.boxplot(['A','B'])"}
{"cell_type":"markdown","metadata":{},"source":"This didn't really help: it maybe spotted an outlier in the 'B' data, but it has way too many in the 'A' data. Let's try going at this another way. We'll start with the 'A' data.\n\nFirst we'll calculate the mean $\\mu$ and standard deviation $\\sigma$ for the data. Then we'll scale the data like this: we subtract the mean from each data point $x_i$ and divide by the standard deviation.\n\n$\\frac{x_i - \\mu}{\\sigma}$\n\nThis gives us a scaled \"distance\" between each data point and the mean where the scale is the standard deviation. Let's run this for the 'A' data."}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7ff5208a32e8>"},"execution_count":5,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::548b5542-52cc-4570-bc08-92c11fb659fd","text/plain":"<matplotlib.figure.Figure at 0x7ff5401fb4a8>"},"metadata":{},"output_type":"display_data"}],"source":"X=dfa['A'].values.reshape(-1,1)\nxv= range(0,len(X))\nmean = X.mean()\nstdev = X.std()\n\n# Now calculate the scaled distance (which is always a positive number, so we take the absolute value of the distance)\ndist = np.abs((X - mean)/stdev)\n\n#Plot the data\nfig, ax1 = plt.subplots()\nax1.scatter(xv,X,marker='^')\nax1.set_xlabel('Point Number')\nax1.set_ylabel('A Points')\nax2 = ax1.twinx()\nax2.set_ylabel('Scaled Distance',color='m')\nax2.scatter(xv,dist,color='m',marker='v')"}
{"cell_type":"markdown","metadata":{},"source":"Again, there is one point in the 'A' data that stands out. That is certainly an outlier. But we'd like to use a tool that does this scaling for us and will let us take the 'B' data into consideration, too. Fortunately there is a tool: the Mahalanobis distance [explanation here](https://en.wikipedia.org/wiki/Mahalanobis_distance). Let's implement this tool using just the 'A' column to compare it to our calculation."}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7ff50adfe048>"},"execution_count":6,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::5c66eae5-02bf-4bf8-b549-c491f7b752fa","text/plain":"<matplotlib.figure.Figure at 0x7ff520799390>"},"metadata":{},"output_type":"display_data"}],"source":"from sklearn.covariance import EmpiricalCovariance\n\n# We initialize the model and fit in one step here.\nemp_covA = EmpiricalCovariance().fit(X)\n\n# The Mahalanobis function returns the distance squared, so we take the square root.\nmahal_dist = np.sqrt(emp_covA.mahalanobis(X))\n\n# And plot the data\nfig, ax1 = plt.subplots()\nax1.scatter(xv,X,marker='^')\nax1.set_xlabel('Point Number')\nax1.set_ylabel('A Points')\nax2 = ax1.twinx()\nax2.set_ylabel('Mahalanobis Distance',color='m')\nax2.scatter(xv,mahal_dist,color='m',marker='v')"}
{"cell_type":"markdown","metadata":{},"source":"This looks exactly like our manual distance calculation. That's good. Now let's extend this to use both columns as the inputs. We'll again calculate the distances and plot them for each point."}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7ff50ad75fd0>"},"execution_count":7,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::8fe32871-be0a-4cb4-a04d-c8cad2b7bcd2","text/plain":"<matplotlib.figure.Figure at 0x7ff5231527f0>"},"metadata":{},"output_type":"display_data"}],"source":"XAB=dfa[['A','B']].values\nemp_covAB = EmpiricalCovariance().fit(XAB)\nmahal_dist2 = np.sqrt(emp_covAB.mahalanobis(XAB))\nfig, ax1 = plt.subplots()\nax1.scatter(xv,X,marker='^')\nax1.set_xlabel('Point Number')\nax1.set_ylabel('A Points')\nax2 = ax1.twinx()\nax2.set_ylabel('Mahalanobis Distance',color='m')\nax2.scatter(xv,mahal_dist2,color='m',marker='v')"}
{"cell_type":"markdown","metadata":{},"source":"Now we can see there are a couple of points that have a Mahalanobis distance bigger than 4. That's a good indication that they are outlier for this dataset. We can plot the data as a general scatter plot and then overlay contour lines for the different Mahalanobis distances from the mean. That will let us see how the distances compare."}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<a list of 8 text.Text objects>"},"execution_count":8,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::f5c1028a-66e8-42af-8c02-dd005aa0548c","text/plain":"<matplotlib.figure.Figure at 0x7ff50acd0d68>"},"metadata":{},"output_type":"display_data"}],"source":"trainfig, ax = plt.subplots()\n\nax.scatter(dfa['A'].values,dfa['B'].values,color='black')\nax.set_title(\"Mahalanobis distances\")\n\n# Show contours of the distance functions\nxx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))\nzz = np.c_[xx.ravel(), yy.ravel()]\n\nmahal_emp_cov = emp_covAB.mahalanobis(zz)\nmahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\nemp_cov_contour = ax.contour(xx, yy, np.sqrt(mahal_emp_cov),\n                                  cmap=plt.cm.PuBu_r,\n                                  linestyles='dashed',\n                                  levels=range(8))\nplt.clabel(emp_cov_contour, inline=1, fontsize=10)"}
{"cell_type":"markdown","metadata":{},"source":"Now we can see that there are a couple of points with a distance bigger than 4.0. This cutoff is slightly arbitrary- we choose how big or small we want to make it depending on the data and what we are doing with it. For now, we'll set the cutoff distance and then create a cleaned dataset where the distances are less than this cutoff."}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Initial number of points: 200\nCleaned number of points: 198\n"}],"source":"cutoff_distance = 4.0\n\ndfa['dist'] = mahal_dist2\ndfacl = dfa[dfa['dist'] <= cutoff_distance]\nprint(\"Initial number of points: {}\".format(len(dfa.index)))\nprint(\"Cleaned number of points: {}\".format(len(dfacl.index)))"}
{"cell_type":"markdown","metadata":{},"source":"### Outlier Category\n\nInstead of removing the points, we could label them or otherwise mark them. We'll do this by creating a True-False categorical column that marks the potential outliers as True."}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"A           float64\nB           float64\ndist        float64\noutlier    category\ndtype: object\n"},{"data":{"text/html":"smc-blob::9af0cf07-09e3-4433-9ed1-cd18d873dc2e","text/plain":"          A         B      dist outlier\n0  3.379122 -2.227508  0.946078   False\n1  4.895166 -1.991730  0.838179   False\n2  4.704226 -2.630269  2.221110   False\n3  2.069320 -2.607981  2.497841   False\n4  2.400303 -2.419060  1.906594   False"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"dfa['outlier'] = (dfa['dist'] > cutoff_distance).astype('category')\nprint(dfa.dtypes)\ndfa.head()"}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"## Functional Outlier Detection\n\n\nSo that first set of data was fairly easy to work with: the distribution of data points looked Gaussian (it was) and so the outliers were easy to detect using methods that depend on normal distributions. What if our data aren't normally distributed? Let's look at another fake dataset that has a couple of outliers."}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff50acd8f98>"},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::3e726597-577d-45aa-87a2-2d32d9cd1f98","text/plain":"<matplotlib.figure.Figure at 0x7ff52480f898>"},"metadata":{},"output_type":"display_data"}],"source":"dfb = pd.read_csv(\"Class08_fakedata2.csv\")\ndfb.plot.scatter(x='x',y='y')"}
{"cell_type":"markdown","metadata":{},"source":"We can clearly see that there are two points that don't fit the pattern. How do we identify them? We could try a boxplot."}
{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff50ab98908>"},"execution_count":12,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::ba37ee56-de0c-4033-bc67-6001d1460d97","text/plain":"<matplotlib.figure.Figure at 0x7ff50aba6748>"},"metadata":{},"output_type":"display_data"}],"source":"dfb.boxplot('y')"}
{"cell_type":"markdown","metadata":{},"source":"That doesn't get us anywhere. How about trying the Mahalanobis distance?"}
{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7ff50ab38eb8>"},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::5db52e22-396e-45b8-adc4-dc5cef3f027b","text/plain":"<matplotlib.figure.Figure at 0x7ff510ed16d8>"},"metadata":{},"output_type":"display_data"}],"source":"xy=dfb[['x','y']].values\nemp_covxy = EmpiricalCovariance().fit(xy)\nmahal_distxy = np.sqrt(emp_covxy.mahalanobis(xy))\n\ntrainfig, ax = plt.subplots()\nax.scatter(dfb['x'].values,dfb['y'].values,color='black')\nax.set_title(\"Mahalanobis distances\")\n\n# Show contours of the distance functions\nxx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))\nzz = np.c_[xx.ravel(), yy.ravel()]\n\nmahal_emp_cov = emp_covxy.mahalanobis(zz)\nmahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\nemp_cov_contour = ax.contour(xx, yy, np.sqrt(mahal_emp_cov),\n                                  cmap=plt.cm.PuBu_r,\n                                  linestyles='dashed')\nax.clabel(emp_cov_contour, inline=1, fontsize=10)\nax.set_xlabel('x')\nax.set_ylabel('y')"}
{"cell_type":"markdown","metadata":{},"source":"That isn't helpful, either. We need a new technique. We'll start by creating a machine learning model to predict the data. In this case we'll use a support vector regression. It should be weighted to try and fit the majority of the data which look like they are good. We'll also print out the `score` for the model fit. This is the accuracy score that we've used before."}
{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Score: 0.9870568575200664\n"},{"data":{"image/png":"smc-blob::049c9345-85c5-4290-8c2b-20d64528e123","text/plain":"<matplotlib.figure.Figure at 0x7ff5401f74a8>"},"metadata":{},"output_type":"display_data"}],"source":"from sklearn.svm import SVR\nsvrmodel = SVR()\nsvrmodel.fit(dfb['x'].values.reshape(-1,1), dfb['y'].values)\n\nX_plot =np.linspace(-1, 1, 1000)\nY_pred = svrmodel.predict(X_plot[:,None])\n\nfig, ax = plt.subplots()\n# First plot our points\nax.scatter(dfb['x'].values,dfb['y'].values,color='black')\nax.plot(X_plot,Y_pred,c='r')\nax.set_xlabel('x')\nax.set_ylabel('y')\n\nprint(\"Score: {}\".format(svrmodel.score(dfb['x'].values.reshape(-1,1),dfb['y'].values)))"}
{"cell_type":"markdown","metadata":{},"source":"Like we hoped, the model looks like it follows most of the data. Now we can look to see if we can find those outliers. We'll plot the model predictions against the actual `y` values."}
{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.text.Text at 0x7ff50aa47e10>"},"execution_count":15,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::5883dc6a-bd9c-4046-a9db-e7c5759dc506","text/plain":"<matplotlib.figure.Figure at 0x7ff50ab10160>"},"metadata":{},"output_type":"display_data"}],"source":"pred = svrmodel.predict(dfb['x'].values.reshape(-1,1))\nplt.scatter(dfb['y'].values,pred,color='blue')\nplt.xlabel('Actual y values')\nplt.ylabel('Model predicted y values')"}
{"cell_type":"markdown","metadata":{},"source":"If the model prediction matches the actual value, the data point lies on a line with a slope of `1`. We see that most of the data points are on that line! And there are two points that are off the line - those are our outliers. Another way to visualize this is to calculate the **residuals**: the difference between the actual `y` values and the predicted `y` values. We'll create a new column of the residuals and plot them."}
{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff50a99df60>"},"execution_count":16,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::d4068a84-79b3-4f4d-afde-6c6c7c674009","text/plain":"<matplotlib.figure.Figure at 0x7ff50a9aeb00>"},"metadata":{},"output_type":"display_data"}],"source":"dfb['residual'] = np.abs(dfb['y'].values - pred)\n\ndfb.plot.scatter(x='x',y='residual')"}
{"cell_type":"markdown","metadata":{},"source":"Now we can sort the data by the residual column and look for the largest values."}
{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/html":"smc-blob::e7ae0cf4-d86e-4d9f-a9c1-15cd05e80e4a","text/plain":"        x         y  residual\n50  -0.50 -1.500000  0.577333\n150  0.50  0.500000  0.411061\n149  0.49  1.038593  0.128300\n156  0.56  1.017611  0.119498\n1   -0.99 -0.007512  0.117681"},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":"dfb.sort_values('residual',ascending=False).head()"}
{"cell_type":"markdown","metadata":{},"source":"Now we'll create a new dataset where we select only the data where the residuals are less than a threshold value. We'll try fitting this data to see if our model has improved without the outliers."}
{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Score: 0.9896101260044042\n"},{"data":{"image/png":"smc-blob::a7e4cfaa-14ee-4aa7-b7ad-35e916e394dc","text/plain":"<matplotlib.figure.Figure at 0x7ff50abc60f0>"},"metadata":{},"output_type":"display_data"}],"source":"threshold = 0.3\n\ndfbcl = dfb[dfb['residual'] <= threshold]\n\nsvrmodel2 = SVR(C=10,gamma=1)\nsvrmodel2.fit(dfbcl['x'].values.reshape(-1,1), dfbcl['y'].values)\n\nY_pred = svrmodel2.predict(X_plot[:,None])\n\nfig, ax = plt.subplots()\n# First plot our points\nax.scatter(dfbcl['x'].values,dfbcl['y'].values,color='black')\nax.plot(X_plot,Y_pred,c='r')\nprint(\"Score: {}\".format(svrmodel2.score(dfb['x'].values.reshape(-1,1),dfb['y'].values)))"}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"It looks like the score went up a little bit and the model looks a little better. Certainly we don't have those outliers any more!\n\n## In-class Activity\n\nTake a look at the fake data in `Class08_fakedata3.csv`. There are 3 outlier points. Find them and check with me to see if you found them.\n\n## Assignment\n\nYou may or may not have outliers in your data. Check and see and report on what you found (if anything)."}